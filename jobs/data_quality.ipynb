{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3fa98e5-0108-4702-8b48-d5e684596485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "class DataQuality:\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        self.spark = spark\n",
    "\n",
    "    def standardize_column_names(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Standardizes column names by replacing spaces and hyphens with underscores\n",
    "        and converting all names to lowercase.\n",
    "\n",
    "        Parameters:\n",
    "            df (DataFrame): Input DataFrame\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: DataFrame with standardized column names\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If DataFrame is invalid or has no columns\n",
    "            Exception: For unexpected transformation errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate DataFrame\n",
    "            if df is None or not isinstance(df, DataFrame):\n",
    "                raise ValueError(\"Invalid DataFrame provided.\")\n",
    "\n",
    "            if not df.columns:\n",
    "                raise ValueError(\"DataFrame has no columns to standardize.\")\n",
    "\n",
    "            # Standardize names\n",
    "            new_cols = [col.replace(\" \", \"_\").replace(\"-\", \"_\").lower() for col in df.columns]\n",
    "            for old_col, new_col in zip(df.columns, new_cols):\n",
    "                df = df.withColumnRenamed(old_col, new_col)\n",
    "\n",
    "            return df\n",
    "\n",
    "        except ValueError as ve:\n",
    "            # Re-raise validation errors for clarity\n",
    "            raise ve\n",
    "        except Exception as e:\n",
    "            # Wrap and raise any other unexpected errors\n",
    "            raise Exception(f\"Unexpected error while standardizing column names: {e}\") from e\n",
    "    \n",
    "    def filter_not_null(self, df: DataFrame, cols: list) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Filter rows where none of the given columns are null.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): Input Spark DataFrame.\n",
    "            cols (list): List of column names to check for null values.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: Filtered DataFrame where all given columns are non-null.\n",
    "        \"\"\"\n",
    "        if not isinstance(cols, list):\n",
    "            raise TypeError(f\"'cols' must be a list of column names, got {type(cols).__name__}\")\n",
    "    \n",
    "        if not all(isinstance(c, str) for c in cols):\n",
    "            raise TypeError(f\"All column names must be strings. Invalid entries: {[c for c in cols if not isinstance(c, str)]}\")\\\n",
    "\n",
    "        condition = F.lit(True)\n",
    "        for col in cols:\n",
    "            condition = condition & F.col(col).isNotNull()\n",
    "        return df.filter(condition)\n",
    "\n",
    "    def filter_valid_contact(self,df: DataFrame, col: str) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Filter rows with invalid US phone numbers.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): Input Spark DataFrame.\n",
    "            col (str): Column containing phone numbers.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: Filtered DataFrame with only invalid phone numbers.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # --- Input validation ---\n",
    "            if not isinstance(df, DataFrame):\n",
    "                raise TypeError(\"df must be a pyspark.sql.DataFrame\")\n",
    "\n",
    "            if not isinstance(col, str):\n",
    "                raise TypeError(\"col must be a string\")\n",
    "\n",
    "            if col not in df.columns:\n",
    "                raise ValueError(f\"Column '{col}' not found in DataFrame\")\n",
    "\n",
    "            field_type = dict(df.dtypes)[col]\n",
    "            if field_type != \"string\":\n",
    "                raise ValueError(f\"Column '{col}' must be of type string, found {field_type}\")\n",
    "            # Regex for valid US phone numbers\n",
    "            valid_phone_regex = r\"^(\\+1\\s?)?(\\(?\\d{3}\\)?[\\s.-]?)\\d{3}[\\s.-]?\\d{4}$\"\n",
    "\n",
    "            return df.filter(F.col(col).rlike(valid_phone_regex))\n",
    "        except (TypeError, ValueError) as e:\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            # Unexpected errors â†’ wrap in RuntimeError\n",
    "            raise RuntimeError(f\"Unexpected error in filter_valid_contact: {str(e)}\") from e\n",
    "\n",
    "    def clean_name_column(self,df: DataFrame, col_name: str = \"name\") -> DataFrame:\n",
    "        \"\"\"\n",
    "        Removes special characters and numbers from the given column in a PySpark DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        col_name (str): Column to clean (overwritten)\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: DataFrame with the cleaned column (same name)\n",
    "\n",
    "        Raises:\n",
    "        ValueError: If DataFrame or column is invalid\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate DataFrame\n",
    "            if df is None or not isinstance(df, DataFrame):\n",
    "                raise ValueError(\"Invalid DataFrame provided.\")\n",
    "\n",
    "            # Validate column existence\n",
    "            if col_name not in df.columns:\n",
    "                raise ValueError(f\"Column '{col_name}' does not exist in the DataFrame.\")\n",
    "\n",
    "            # Overwrite the same column with cleaned values\n",
    "            cleaned_df = df.withColumn(\n",
    "                col_name,\n",
    "                F.trim(F.regexp_replace(F.col(col_name), \"[^A-Za-z ]\", \"\"))\n",
    "            )\n",
    "            return cleaned_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error while cleaning column '{col_name}': {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def filter_valid_orders(self,df: DataFrame, order_col: str = \"order_date\", ship_col: str = \"ship_date\") -> DataFrame:\n",
    "        \"\"\"\n",
    "        Filters records where order_date is less than ship_date.\n",
    "\n",
    "        Parameters:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        order_col (str): Column name for order date\n",
    "        ship_col (str): Column name for ship date\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: Filtered DataFrame with valid records\n",
    "\n",
    "        Raises:\n",
    "        ValueError: If DataFrame or required columns are invalid\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate DataFrame\n",
    "            if df is None or not isinstance(df, DataFrame):\n",
    "                raise ValueError(\"Invalid DataFrame provided.\")\n",
    "\n",
    "            # Validate column existence\n",
    "            missing_cols = [col for col in [order_col, ship_col] if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns in DataFrame: {missing_cols}\")\n",
    "\n",
    "            # Filter where order_date < ship_date\n",
    "            filtered_df = df.filter(F.col(order_col) < F.col(ship_col))\n",
    "            return filtered_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error while filtering records: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def filter_positive_values(self, df: DataFrame, col_names: list) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Filters records where the given columns have only positive values.\n",
    "\n",
    "        Parameters:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        col_names (list): List of columns to filter\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: Filtered DataFrame with only positive values for the given columns\n",
    "\n",
    "        Raises:\n",
    "        ValueError: If DataFrame is invalid, columns missing, or not numeric\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate DataFrame\n",
    "            if df is None or not isinstance(df, DataFrame):\n",
    "                raise ValueError(\"Invalid DataFrame provided.\")\n",
    "\n",
    "            # Validate column list\n",
    "            if not isinstance(col_names, list) or not col_names:\n",
    "                raise ValueError(\"Parameter 'col_names' must be a non-empty list.\")\n",
    "\n",
    "            # Build filter conditions\n",
    "            filter_condition = None\n",
    "            for col_name in col_names:\n",
    "                # Validate column existence\n",
    "                if col_name not in df.columns:\n",
    "                    raise ValueError(f\"Column '{col_name}' does not exist in the DataFrame.\")\n",
    "\n",
    "                # Validate numeric type\n",
    "                col_type = dict(df.dtypes)[col_name]\n",
    "                numeric_types = (\"int\", \"bigint\", \"double\", \"float\", \"decimal\", \"long\", \"smallint\")\n",
    "                if not any(nt in col_type for nt in numeric_types):\n",
    "                    raise ValueError(f\"Column '{col_name}' must be numeric, found type '{col_type}'.\")\n",
    "\n",
    "                # Add positive value condition\n",
    "                condition = F.col(col_name) > 0\n",
    "                filter_condition = condition if filter_condition is None else (filter_condition & condition)\n",
    "\n",
    "            # Apply combined filter\n",
    "            filtered_df = df.filter(filter_condition)\n",
    "            return filtered_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error while filtering positive values for columns {col_names}: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f1f0206-8963-4139-83c6-097466d66001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_quality",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
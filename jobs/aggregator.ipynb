{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a979c168-e3e5-4681-a2e8-c1369073698a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, sum as spark_sum,round\n",
    "from pyspark.sql.types import NumericType\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "\n",
    "class Aggregator:\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        self.spark = spark\n",
    "        \n",
    "    def aggregate_profit_by_year(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Aggregate profit by year.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): Input Spark DataFrame with columns 'year' and 'profit'.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: Aggregated DataFrame with profit sum per year (rounded to 2 decimals).\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If df is not a Spark DataFrame or if profit column is not numeric.\n",
    "            ValueError: If required columns are missing or DataFrame is empty.\n",
    "        \"\"\"\n",
    "        # --- Validate input type ---\n",
    "        if not isinstance(df, DataFrame):\n",
    "            raise TypeError(\"Input must be a Spark DataFrame\")\n",
    "\n",
    "        # --- Validate DataFrame not empty ---\n",
    "        if not df.columns:\n",
    "            raise ValueError(\"Input DataFrame is empty (no columns)\")\n",
    "\n",
    "        # --- Validate required columns exist ---\n",
    "        required_cols = {\"year\", \"profit\"}\n",
    "        df_cols = set(df.columns)\n",
    "        missing = required_cols - df_cols\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "        # --- Validate profit column is numeric ---\n",
    "        profit_field = [f for f in df.schema.fields if f.name == \"profit\"][0]\n",
    "        if not isinstance(profit_field.dataType, NumericType):\n",
    "            raise TypeError(\"Column 'profit' must be numeric\")\n",
    "\n",
    "        # --- Perform aggregation ---\n",
    "        try:\n",
    "            return df.groupBy(\"year\").agg(\n",
    "                round(spark_sum(\"profit\"), 2).alias(\"profit_sum\")\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to aggregate profit by year: {str(e)}\")\n",
    "\n",
    "    def aggregate_profit_by_customer(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Aggregate profit by customer.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): Input Spark DataFrame with columns \n",
    "                            'customer_id', 'customer_name', and 'profit'.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: Aggregated DataFrame with profit sum per customer (rounded to 2 decimals).\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If df is not a Spark DataFrame or if 'profit' column is not numeric.\n",
    "            ValueError: If required columns are missing or DataFrame is empty.\n",
    "            RuntimeError: If Spark aggregation fails.\n",
    "        \"\"\"\n",
    "        # --- Validate input type ---\n",
    "        if not isinstance(df, DataFrame):\n",
    "            raise TypeError(\"Input must be a Spark DataFrame\")\n",
    "\n",
    "        # --- Validate DataFrame not empty ---\n",
    "        if not df.columns:\n",
    "            raise ValueError(\"Input DataFrame is empty (no columns)\")\n",
    "\n",
    "        # --- Validate required columns exist ---\n",
    "        required_cols = {\"customer_id\", \"customer_name\", \"profit\"}\n",
    "        df_cols = set(df.columns)\n",
    "        missing = required_cols - df_cols\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "        # --- Validate profit column is numeric ---\n",
    "        profit_field = [f for f in df.schema.fields if f.name == \"profit\"][0]\n",
    "        if not isinstance(profit_field.dataType, NumericType):\n",
    "            raise TypeError(\"Column 'profit' must be numeric\")\n",
    "\n",
    "        # --- Perform aggregation ---\n",
    "        try:\n",
    "            return df.groupBy(\"customer_id\", \"customer_name\").agg(\n",
    "                round(spark_sum(\"profit\"), 2).alias(\"profit_sum\")\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to aggregate profit by customer: {str(e)}\")\n",
    "                \n",
    "\n",
    "    def aggregate_profit_by_year_and_category(self, df: DataFrame) -> DataFrame:\n",
    "        try:\n",
    "            # --- Validate input type ---\n",
    "            if not isinstance(df, DataFrame):\n",
    "                raise TypeError(\"Input must be a Spark DataFrame\")\n",
    "\n",
    "            # --- Validate required columns exist ---\n",
    "            required_cols = {\"year\", \"product_category\", \"profit\"}\n",
    "            df_cols = set(df.columns)\n",
    "            missing = required_cols - df_cols\n",
    "            if missing:\n",
    "                raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "            # --- Validate profit is numeric ---\n",
    "            profit_field = [f for f in df.schema.fields if f.name == \"profit\"][0]\n",
    "            if not isinstance(profit_field.dataType, NumericType):\n",
    "                raise TypeError(\"Column 'profit' must be numeric\")\n",
    "\n",
    "            # --- Perform aggregation ---\n",
    "            return df.groupBy(\"year\", \"product_category\").agg(\n",
    "                round(spark_sum(\"profit\"), 2).alias(\"profit_sum\")\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch unexpected errors and re-raise with context\n",
    "            raise RuntimeError(f\"Error in aggregate_profit_by_year_and_category: {str(e)}\") from e\n",
    "\n",
    "\n",
    "    def aggregate_profit(self, enriched_orders: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Aggregate profit by year, category, sub-category, and customer.\n",
    "\n",
    "        Args:\n",
    "            enriched_orders (DataFrame): Input Spark DataFrame containing\n",
    "                'year', 'category', 'Sub_Category', 'customer_id',\n",
    "                'customer_name', and 'profit'.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: Aggregated DataFrame with profit sums per grouping\n",
    "                    (rounded to 2 decimals).\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If enriched_orders is not a Spark DataFrame or 'profit' is not numeric.\n",
    "            ValueError: If required columns are missing or DataFrame is empty.\n",
    "            RuntimeError: If Spark aggregation fails.\n",
    "        \"\"\"\n",
    "        # --- Validate input type ---\n",
    "        if not isinstance(enriched_orders, DataFrame):\n",
    "            raise TypeError(\"Input must be a Spark DataFrame\")\n",
    "\n",
    "        # --- Validate DataFrame is not empty ---\n",
    "        if not enriched_orders.columns:\n",
    "            raise ValueError(\"Input DataFrame is empty (no columns)\")\n",
    "\n",
    "        # --- Validate required columns ---\n",
    "        required_cols = {\"year\", \"category\", \"sub_category\", \"customer_id\", \"profit\"}\n",
    "        df_cols = set(enriched_orders.columns)\n",
    "        missing = required_cols - df_cols\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "        # --- Validate profit column type ---\n",
    "        profit_field = [f for f in enriched_orders.schema.fields if f.name == \"profit\"][0]\n",
    "        if not isinstance(profit_field.dataType, NumericType):\n",
    "            raise TypeError(\"Column 'profit' must be numeric\")\n",
    "\n",
    "        # --- Perform aggregation ---\n",
    "        try:\n",
    "            return enriched_orders.groupBy(\n",
    "                \"year\", \"category\", \"Sub_Category\",\n",
    "                \"customer_id\",\"customer_name\"\n",
    "            ).agg(round(spark_sum(col(\"profit\")), 2).alias(\"profit_sum\"))\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to aggregate profit: {str(e)}\")\n",
    "\n",
    "\n",
    "    def aggregate_with_query(self,  query: str) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Run a SQL query on a table and return result.\n",
    "\n",
    "        Args:\n",
    "            query (str): SQL query to execute.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: Result of the SQL query.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If query inputs is not strings.\n",
    "            ValueError: If query is empty.\n",
    "            RuntimeError: If SQL execution fails.\n",
    "        \"\"\"\n",
    "        # --- Input type validations ---\n",
    "        if not isinstance(query, str):\n",
    "            raise TypeError(\"Query must be a string\")\n",
    "\n",
    "        # --- Validate query ---\n",
    "        if not query.strip():\n",
    "            raise ValueError(\"Query cannot be empty\")\n",
    "\n",
    "        try:\n",
    "            # Run the SQL query\n",
    "            return spark.sql(query)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to execute SQL query: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "126f966b-473c-4a97-9a36-cd02fa34a527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "aggregator",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
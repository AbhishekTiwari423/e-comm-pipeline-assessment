{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e0f4be4-e468-491d-9a5c-2cc168440dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../jobs/reader\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4861c8e5-aa58-4415-8804-9d0947444775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../jobs/data_quality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0bf3679-45b6-4090-9176-a6cbb7101189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../jobs/transformer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da9a7f17-83c1-48e2-90d6-842e7a57f62f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../jobs/aggregator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9598272f-729e-4ca4-a650-0f5e4fff228a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../jobs/utility\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69bcb3e9-1f84-4319-904a-83277e855f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting e-commerce-pipeline\n+-----------+-------------+--------------------+-----------------+--------------------+--------+-------------+----------+-------+-----------+-------+\n|customer_id|customer_name|               email|            phone|             address| segment|      country|      city|  state|postal_code| region|\n+-----------+-------------+--------------------+-----------------+--------------------+--------+-------------+----------+-------+-----------+-------+\n|   AA-10315|   Alex Avila|josephrice131@gma...|     680-261-2092|91773 Miller Shoa...|Consumer|United States|Round Rock|  Texas|      78664|Central|\n|   AA-10375| Allen Armold|garymoore386@gmai...|221.945.4191x8872|6450 John Lodge\\n...|Consumer|United States|   Atlanta|Georgia|      30318|  South|\n+-----------+-------------+--------------------+-----------------+--------------------+--------+-------------+----------+-------+-----------+-------+\nonly showing top 2 rows\n\n+--------------+----------+----------+-----------+---------------+-------------+---------------+---------------+------------+------+----+\n|      order_id|order_date| ship_date|customer_id|  customer_name|      country|     product_id|       category|sub_category|profit|year|\n+--------------+----------+----------+-----------+---------------+-------------+---------------+---------------+------------+------+----+\n|CA-2014-100293|2014-03-14|2014-03-18|   NF-18475|Neil Franzsisch|United States|OFF-PA-10000176|Office Supplies|       Paper| 31.87|2014|\n|CA-2014-100391|2014-05-25|2014-05-29|   BW-11065|  Barry Weirich|United States|OFF-PA-10001471|Office Supplies|       Paper|  6.73|2014|\n|CA-2014-100678|2014-04-18|2014-04-22|   KM-16720|   Kunst Miller|United States|OFF-EN-10000056|Office Supplies|   Envelopes| 50.41|2014|\n|CA-2014-100706|2014-12-16|2014-12-18|   LE-16810|Laurel Elliston|United States|TEC-AC-10001314|     Technology| Accessories|   8.0|2014|\n|CA-2014-100762|2014-11-24|2014-11-29|   NG-18355|     Nat Gilpin|United States|OFF-LA-10003930|Office Supplies|      Labels| 96.34|2014|\n+--------------+----------+----------+-----------+---------------+-------------+---------------+---------------+------------+------+----+\n\naggregated profit by categories like year, product category, sub category and customer :\n+----+---------------+------------+-----------+----------------+----------+\n|year|       category|Sub_Category|customer_id|   customer_name|profit_sum|\n+----+---------------+------------+-----------+----------------+----------+\n|2014|Office Supplies|         Art|   JD-15895|Jonathan Doherty|       4.0|\n|2014|Office Supplies|  Appliances|   SF-20200|    Sarah Foster|     58.03|\n|2014|      Furniture| Furnishings|   AP-10915|  Arthur Prichep|      53.2|\n|2015|      Furniture|      Chairs|   LC-16960| Lindsay Castell|    267.67|\n|2015|Office Supplies|     Binders|   RP-19855|        Roy Phan|      26.6|\n+----+---------------+------------+-----------+----------------+----------+\n\n----------Profit by Year------------------------------\n+----+------------+\n|year|total_profit|\n+----+------------+\n|2014|    17385.28|\n|2015|     18040.4|\n|2016|    37218.06|\n|2017|    29703.98|\n+----+------------+\n\n----------Profit by Year + Product Category------------\n+----+---------------+------------+\n|year|       category|total_profit|\n+----+---------------+------------+\n|2014|     Technology|     9799.62|\n|2014|      Furniture|     1266.73|\n|2014|Office Supplies|     6318.93|\n|2015|Office Supplies|     8893.74|\n|2015|      Furniture|     1259.51|\n+----+---------------+------------+\nonly showing top 5 rows\n\n--------------Profit by Customer-------------------\n+-----------+----------------+------------+\n|customer_id|   customer_name|total_profit|\n+-----------+----------------+------------+\n|   VP-21760|Victoria Pisteka|       88.89|\n|   PR-18880|    Patrick Ryan|       40.56|\n|   JK-15640|        Jim Kriz|      183.86|\n|   RB-19435| Richard Bierner|       74.96|\n|   CD-12280|Christina DeMoss|        2.96|\n+-----------+----------------+------------+\nonly showing top 5 rows\n\n--------------Profit by Customer + Year-------------\n+----+-----------+-------------+------------+\n|year|customer_id|customer_name|total_profit|\n+----+-----------+-------------+------------+\n|2014|   ND-18460|  Neil Ducich|       -3.81|\n|2014|   RA-19285| Ralph Arnett|         0.0|\n|2014|   AC-10660|   Anna Chung|        2.89|\n|2014|   TC-21475| Tony Chapman|      -16.47|\n|2014|   MC-17575| Matt Collins|       -6.34|\n+----+-----------+-------------+------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "reader = Reader(spark)\n",
    "transformer = Transformer(spark)\n",
    "dq = DataQuality(spark)\n",
    "aggregator = Aggregator(spark)\n",
    "utility = Utility(spark)\n",
    "\n",
    "\n",
    "def main():\n",
    "        print(\"starting e-commerce-pipeline\")\n",
    "\n",
    "        # Input data paths for datasets\n",
    "        order_path = \"dbfs:/FileStore/shared_uploads/abhi687303@gmail.com/Test/Orders.json\"\n",
    "        customer_data = \"dbfs:/FileStore/shared_uploads/abhi687303@gmail.com/Test/Customer.xlsx\"\n",
    "        product_data = \"dbfs:/FileStore/shared_uploads/abhi687303@gmail.com/Test/Products.csv\"\n",
    "\n",
    "        # Schema definition for datasets-\n",
    "        #---------customer dataset ------------------\n",
    "        customer_schema = StructType([\n",
    "        StructField(\"Customer ID\", StringType(), True),\n",
    "        StructField(\"Customer Name\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"phone\", StringType(), True),\n",
    "        StructField(\"address\", StringType(), True),\n",
    "        StructField(\"Segment\", StringType(), True),\n",
    "        StructField(\"Country\", StringType(), True),\n",
    "        StructField(\"City\", StringType(), True),\n",
    "        StructField(\"State\", StringType(), True),\n",
    "        StructField(\"Postal Code\", StringType(), True),\n",
    "        StructField(\"Region\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        #----------product dataset schema ----------\n",
    "        product_schema = StructType([\n",
    "        StructField(\"Product ID\", StringType(), True),\n",
    "        StructField(\"Category\", StringType(), True),\n",
    "        StructField(\"Sub-Category\", StringType(), True),\n",
    "        StructField(\"Product Name\", StringType(), True),\n",
    "        StructField(\"State\", StringType(), True),\n",
    "        StructField(\"Price per product\", DoubleType(), True)\n",
    "        ])\n",
    "\n",
    "        #-------------Order dataset schema ----------- \n",
    "        order_schema = StructType([\n",
    "        StructField(\"Row ID\", IntegerType(), True),\n",
    "        StructField(\"Order ID\", StringType(), True),\n",
    "        StructField(\"Order Date\", StringType(), True),   \n",
    "        StructField(\"Ship Date\", StringType(), True),    \n",
    "        StructField(\"Ship Mode\", StringType(), True),\n",
    "        StructField(\"Customer ID\", StringType(), True),\n",
    "        StructField(\"Product ID\", StringType(), True),\n",
    "        StructField(\"Quantity\", IntegerType(), True),\n",
    "        StructField(\"Price\", DoubleType(), True),\n",
    "        StructField(\"Discount\", DoubleType(), True),\n",
    "        StructField(\"Profit\", DoubleType(), True)\n",
    "        ])\n",
    "\n",
    "        # Read input data \n",
    "        product_df = reader.read_csv(product_data,True, product_schema)\n",
    "        customer_df = reader.read_excel(customer_data,True, customer_schema)\n",
    "        order_df = reader.read_json(order_path,True, order_schema)\n",
    "        # ----------space removal from column names --------------\n",
    "        clean_customer_df = dq.standardize_column_names(customer_df)\n",
    "        clean_order_df = dq.standardize_column_names(order_df)\n",
    "        clean_product_df = dq.standardize_column_names(product_df)\n",
    "\n",
    "        #================== Task:1. raw table for each dataset ==========================\n",
    "        raw_customer_table = utility.create_delta_table(clean_customer_df, \"raw_customer\")\n",
    "        raw_order_table = utility.create_delta_table(clean_customer_df,\"raw_order\")\n",
    "        raw_product_table = utility.create_delta_table(clean_customer_df,\"raw_product\") \n",
    "\n",
    "\n",
    "        # -----------data quality checks on customer dataset---------------------\n",
    "        valid_cust_df = dq.filter_not_null(clean_customer_df,[\"customer_id\",\"country\"])\n",
    "        valid_cust_df = dq.clean_name_column(valid_cust_df,\"customer_name\")\n",
    "        valid_cust_df = valid_cust_df.dropDuplicates([\"customer_id\"])\n",
    "        valid_cust_df.show(2)\n",
    "\n",
    "        # -----------data quality checks on order dataset------------------------\n",
    "\n",
    "        valid_order_df = dq.filter_not_null(clean_order_df,[\"order_id\",\"order_date\",\"ship_date\",\"customer_id\",\"product_id\",\"quantity\",\n",
    "                                                        \"price\",\"discount\",\"profit\"])\n",
    "        valid_order_df = dq.filter_positive_values(valid_order_df,[\"quantity\",\"price\"])\n",
    "        valid_order_df = dq.filter_valid_orders(valid_order_df,\"order_date\",\"ship_date\")\n",
    "        valid_order_df = valid_order_df.dropDuplicates([\"order_id\"])\n",
    "        # valid_order_df.show(2)\n",
    "\n",
    "        #------------data quality checks on product dataset --------------------\n",
    "        valid_product_df = dq.filter_not_null(clean_product_df,[\"product_id\",\"category\",\"sub_category\",\"product_name\",\"price_per_product\"])\n",
    "        valid_product_df = dq.filter_positive_values(valid_product_df,[\"price_per_product\"])\n",
    "        valid_product_df = valid_product_df.dropDuplicates([\"product_id\"])\n",
    "\n",
    "        #================== Task:2.enrich table for customer and product=====================\n",
    "        enrich_customer_table = utility.create_delta_table(valid_cust_df, \"enrich_customer\")\n",
    "        enrich_product_table = utility.create_delta_table(valid_product_df, \"enrich_product\")\n",
    "\n",
    "        \n",
    "        valid_order_df = transformer.transform_orders(valid_order_df)\n",
    "        valid_cust_df = valid_cust_df.withColumn(\"customer_id\", F.trim(F.col(\"customer_id\")))\n",
    "        valid_order_df = valid_order_df.withColumn(\"customer_id\", F.trim(F.col(\"customer_id\")))\n",
    "        enrich_orders = transformer.enrich_orders(valid_order_df,valid_cust_df,valid_product_df)\n",
    "\n",
    "        #=================== Task:3.Final enrich table with all details======================\n",
    "        enrich_table = utility.create_delta_table(enrich_orders, \"enriched\")\n",
    "        spark.sql(\"select * from enriched limit 5\").show()\n",
    "\n",
    "\n",
    "        #=============== Task:4.Aggregated table various input categories ============================\n",
    "        agg_profit_cat = aggregator.aggregate_profit(enrich_orders)\n",
    "        aggregated_table = utility.create_delta_table(agg_profit_cat, \"aggregated\")\n",
    "        print(\"aggregated profit by categories like year, product category, sub category and customer :\")\n",
    "        spark.sql(\"select * from aggregated limit 5\").show()\n",
    "        # agg_profit_cat.orderBy(\"year\").show(5)\n",
    "\n",
    "\n",
    "        #============= Task:5. Aggregated results using sqls ================================================\n",
    "        print(\"----------Profit by Year------------------------------\")\n",
    "        year_query = \"\"\"SELECT year,round(SUM(profit),2) AS total_profit FROM enriched GROUP BY year ORDER BY year\"\"\"\n",
    "        aggregator.aggregate_with_query(year_query).show(5)\n",
    "        #spark.sql(\"\").show(5)\n",
    "\n",
    "        print(\"----------Profit by Year + Product Category------------\")\n",
    "        query = \"SELECT year,category,round(SUM(profit),2) AS total_profit FROM enriched GROUP BY year, category ORDER BY year\"\n",
    "        aggregator.aggregate_with_query(query).show(5)\n",
    "\n",
    "        print(\"--------------Profit by Customer-------------------\")\n",
    "        query = \"\"\"SELECT customer_id,customer_name, round(SUM(profit),2) AS total_profit FROM enriched GROUP BY customer_id, customer_name\"\"\"\n",
    "        aggregator.aggregate_with_query(query).show(5)\n",
    "\n",
    "\n",
    "        print(\"--------------Profit by Customer + Year-------------\")\n",
    "        query = \"\"\"SELECT year, customer_id,customer_name,round(SUM(profit),2) AS total_profit FROM enriched GROUP BY year, customer_id, \n",
    "        customer_name ORDER BY year \"\"\"\n",
    "        aggregator.aggregate_with_query(query).show(5)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a52960d-3e94-4141-9bf6-8aef0984550e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3350319886817323>, line 152\u001B[0m\n",
       "\u001B[1;32m    145\u001B[0m         spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mSELECT year, customer_id,customer_name,round(SUM(profit),2) AS total_profit FROM enriched\u001B[39m\n",
       "\u001B[1;32m    146\u001B[0m \u001B[38;5;124m                GROUP BY year, customer_id, customer_name\u001B[39m\n",
       "\u001B[1;32m    147\u001B[0m \u001B[38;5;124m                ORDER BY year\u001B[39m\n",
       "\u001B[1;32m    148\u001B[0m \u001B[38;5;124m                limit 10;\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n",
       "\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[0;32m--> 152\u001B[0m     main()\n",
       "\n",
       "File \u001B[0;32m<command-3350319886817323>, line 128\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m    126\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m----------Profit by Year------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    127\u001B[0m year_query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mSELECT year,round(SUM(profit),2) AS total_profit FROM enriched GROUP BY year ORDER BY year\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[0;32m--> 128\u001B[0m aggregate_with_query(year_query)\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n",
       "\u001B[1;32m    129\u001B[0m \u001B[38;5;66;03m#spark.sql(\"\").show(5)\u001B[39;00m\n",
       "\u001B[1;32m    131\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m----------Profit by Year + Product Category------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'aggregate_with_query' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'aggregate_with_query' is not defined"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-3350319886817323>, line 152\u001B[0m\n\u001B[1;32m    145\u001B[0m         spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mSELECT year, customer_id,customer_name,round(SUM(profit),2) AS total_profit FROM enriched\u001B[39m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;124m                GROUP BY year, customer_id, customer_name\u001B[39m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;124m                ORDER BY year\u001B[39m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;124m                limit 10;\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 152\u001B[0m     main()\n",
        "File \u001B[0;32m<command-3350319886817323>, line 128\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m----------Profit by Year------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    127\u001B[0m year_query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mSELECT year,round(SUM(profit),2) AS total_profit FROM enriched GROUP BY year ORDER BY year\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m--> 128\u001B[0m aggregate_with_query(year_query)\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n\u001B[1;32m    129\u001B[0m \u001B[38;5;66;03m#spark.sql(\"\").show(5)\u001B[39;00m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m----------Profit by Year + Product Category------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "\u001B[0;31mNameError\u001B[0m: name 'aggregate_with_query' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dbutils.fs.rm(\"dbfs:/user/hive/warehouse/product\", recurse=True)\n",
    "# dbutils.fs.rm(\"dbfs:/user/hive/warehouse/customer\", recurse=True)\n",
    "# dbutils.fs.rm(\"dbfs:/user/hive/warehouse/order\", recurse=True)\n",
    "# dbutils.fs.rm(\"dbfs:/user/hive/warehouse/enriched_orders\", recurse=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1253290999150305,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
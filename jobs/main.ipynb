{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e0f4be4-e468-491d-9a5c-2cc168440dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../jobs/reader\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4861c8e5-aa58-4415-8804-9d0947444775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../jobs/data_quality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0bf3679-45b6-4090-9176-a6cbb7101189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../jobs/transformer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da9a7f17-83c1-48e2-90d6-842e7a57f62f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../jobs/aggregator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69bcb3e9-1f84-4319-904a-83277e855f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting e-commerce-pipeline\nfinal aggregated data\n+----+----------+\n|year|profit_sum|\n+----+----------+\n|2015|  47304.51|\n|2014|  31687.24|\n|2016|  61267.36|\n|2017| 106333.12|\n+----+----------+\n\nenriched columns : ['order_id', 'order_date', 'ship_date', 'customer_id', 'customer_name', 'country', 'product_id', 'category', 'sub_category', 'profit', 'year']\naggregated profit by category :\n+----+---------------+------------+-----------+--------------+----------+\n|year|       category|Sub_Category|customer_id| customer_name|profit_sum|\n+----+---------------+------------+-----------+--------------+----------+\n|2014|      Furniture| Furnishings|   AP-10915|Arthur Prichep|     54.47|\n|2015|Office Supplies|     Binders|   RP-19855|      Roy Phan|      26.6|\n|2017|Office Supplies|         Art|   TA-21385|  Tom Ashbrook|      2.04|\n|2016|      Furniture|      Chairs|   EB-14110|Eugene Barchas|    -21.72|\n|2017|     Technology|      Phones|   NG-18355|    Nat Gilpin|     18.34|\n+----+---------------+------------+-----------+--------------+----------+\nonly showing top 5 rows\n\n----------Profit by Year------------------------------\n+----+------------+\n|year|total_profit|\n+----+------------+\n|2014|    41445.81|\n|2015|    66112.79|\n|2016|    68505.83|\n|2017|   127467.08|\n+----+------------+\n\n----------Profit by Year + Product Category------------\n+----+---------------+------------+\n|year|       category|total_profit|\n+----+---------------+------------+\n|2017|Office Supplies|    44837.35|\n|2015|           NULL|     1284.84|\n|2015|Office Supplies|    25163.68|\n|2016|     Technology|    24425.39|\n|2014|     Technology|     23477.9|\n+----+---------------+------------+\n\n--------------Profit by Customer-------------------\n+----+-----------+----------------+------------+\n|year|customer_id|   customer_name|total_profit|\n+----+-----------+----------------+------------+\n|2014|   JD-16015|     Joy Daniels|      -10.34|\n|2014|   AC-10660|      Anna Chung|       -4.97|\n|2014|   RD-19810|Ross DeVincentis|       11.29|\n|2014|   JL-15130|     Jack Lebron|        9.87|\n|2014|   ME-17320|   Maria Etezadi|     1617.56|\n+----+-----------+----------------+------------+\nonly showing top 5 rows\n\n--------------Profit by Customer + Year-------------\n+----+-----------+----------------+------------+\n|year|customer_id|   customer_name|total_profit|\n+----+-----------+----------------+------------+\n|2014|   JD-16015|     Joy Daniels|      -10.34|\n|2014|   AC-10660|      Anna Chung|       -4.97|\n|2014|   RD-19810|Ross DeVincentis|       11.29|\n|2014|   JL-15130|     Jack Lebron|        9.87|\n|2014|   ME-17320|   Maria Etezadi|     1617.56|\n+----+-----------+----------------+------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "reader = Reader(spark)\n",
    "transformer = Transformer(spark)\n",
    "dq = DataQuality(spark)\n",
    "aggregator = Aggregator(spark)\n",
    "\n",
    "def main():\n",
    "    print(\"starting e-commerce-pipeline\")\n",
    "\n",
    "    # Input data paths for datasets\n",
    "    order_path = \"dbfs:/FileStore/shared_uploads/abhi687303@gmail.com/Test/Orders.json\"\n",
    "    customer_data = \"dbfs:/FileStore/shared_uploads/abhi687303@gmail.com/Test/Customer.xlsx\"\n",
    "    product_data = \"dbfs:/FileStore/shared_uploads/abhi687303@gmail.com/Test/Products.csv\"\n",
    "\n",
    "    # Schema definition for datasets-\n",
    "    #---------customer dataset ------------------\n",
    "    customer_schema = StructType([\n",
    "    StructField(\"Customer ID\", StringType(), True),\n",
    "    StructField(\"Customer Name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"Segment\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Postal Code\", StringType(), True),\n",
    "    StructField(\"Region\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    #----------product dataset schema ----------#\n",
    "    product_schema = StructType([\n",
    "    StructField(\"Product ID\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"Sub-Category\", StringType(), True),\n",
    "    StructField(\"Product Name\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Price per product\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "    #-------------Order dataset schema --------------- \n",
    "    order_schema = StructType([\n",
    "        StructField(\"Row ID\", IntegerType(), True),\n",
    "        StructField(\"Order ID\", StringType(), True),\n",
    "        StructField(\"Order Date\", StringType(), True),   \n",
    "        StructField(\"Ship Date\", StringType(), True),    \n",
    "        StructField(\"Ship Mode\", StringType(), True),\n",
    "        StructField(\"Customer ID\", StringType(), True),\n",
    "        StructField(\"Product ID\", StringType(), True),\n",
    "        StructField(\"Quantity\", IntegerType(), True),\n",
    "        StructField(\"Price\", DoubleType(), True),\n",
    "        StructField(\"Discount\", DoubleType(), True),\n",
    "        StructField(\"Profit\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "    # Read input data \n",
    "    product_df = reader.read_csv(product_data,True, product_schema)\n",
    "    customer_df = reader.read_excel(customer_data,True, customer_schema)\n",
    "    order_df = reader.read_json(order_path,True, order_schema)\n",
    "\n",
    "    clean_customer_df = dq.standardize_column_names(customer_df)\n",
    "    clean_order_df = dq.standardize_column_names(order_df)\n",
    "    clean_product_df = dq.standardize_column_names(product_df)\n",
    "\n",
    "    # ---- Save raw tables ------------\n",
    "    # clean_product_df.write.format(\"delta\").saveAsTable(\"product\")\n",
    "    # clean_customer_df.write.format(\"delta\").saveAsTable(\"customer\")\n",
    "    # clean_order_df.write.format(\"delta\").saveAsTable(\"order\")\n",
    "\n",
    "    # ---------------data quality checks on customer dataset---------------------\n",
    "    valid_cust_df = dq.filter_not_null(clean_customer_df,[\"customer_id\",\"country\"])\n",
    "    valid_cust_df = dq.clean_name_column(valid_cust_df,\"customer_name\")\n",
    "    # valida_cust_df.show(2)\n",
    "\n",
    "    # ----------------data quality checks on order dataset-------------------------\n",
    "    \n",
    "    valid_order_df = dq.filter_not_null(clean_order_df,[\"order_id\",\"order_date\",\"ship_date\",\"customer_id\",\"product_id\",\"quantity\",\n",
    "                                                        \"price\",\"discount\",\"profit\"])\n",
    "    valid_order_df = dq.filter_positive_values(valid_order_df,[\"quantity\",\"price\"])\n",
    "    valid_order_df = dq.filter_valid_orders(valid_order_df,\"order_date\",\"ship_date\")\n",
    "    # valid_order_df.show(2)\n",
    "\n",
    "    #----------------data quality checks on product dataset --------------------\n",
    "    valid_product_df = dq.filter_not_null(clean_product_df,[\"product_id\",\"category\",\"sub_category\",\"product_name\",\"price_per_product\"])\n",
    "    valid_product_df = dq.filter_positive_values(clean_product_df,[\"price_per_product\"])\n",
    "\n",
    "    #---------- Enrich order- with joins to get all details at one place for aggregation-----------\n",
    "    valid_order_df = transformer.transform_orders(valid_order_df)\n",
    "    valid_cust_df = valid_cust_df.withColumn(\"customer_id\", F.trim(F.col(\"customer_id\")))\n",
    "    valid_order_df = valid_order_df.withColumn(\"customer_id\", F.trim(F.col(\"customer_id\")))\n",
    "    enrich_orders = transformer.enrich_orders(valid_order_df,valid_cust_df,valid_product_df)\n",
    "\n",
    "    # ----------------save enrich table for sql queries--------------\n",
    "    # enrich_orders.write.format(\"delta\").saveAsTable(\"enriched_orders\")\n",
    "\n",
    "    #------------ Get final results results ------------------\n",
    "    agg_results = aggregator.aggregate_profit_by_year(enrich_orders)\n",
    "    print(\"final aggregated data\")\n",
    "    agg_results.show(10)\n",
    "    print(\"enriched columns :\",enrich_orders.columns)\n",
    "    agg_profit_cat = aggregator.aggregate_profit(enrich_orders)\n",
    "\n",
    "    # save aggregated table by categories\n",
    "    # agg_profit_cat.write.format(\"delta\").saveAsTable(\"aggregated_table\")\n",
    "    print(\"aggregated profit by category :\")\n",
    "    agg_profit_cat.show(5)\n",
    "\n",
    "\n",
    "    #--- Aggregation results using sqls---------------\n",
    "    print(\"----------Profit by Year------------------------------\")\n",
    "    spark.sql(\"SELECT year,round(SUM(profit),2) AS total_profit FROM enriched_orders GROUP BY year ORDER BY year\").show(5)\n",
    "\n",
    "    print(\"----------Profit by Year + Product Category------------\")\n",
    "    spark.sql(\"\"\"SELECT year,category,round(SUM(profit),2) AS total_profit FROM enriched_orders\n",
    "            GROUP BY year, category\n",
    "            --ORDER BY year, category\n",
    "            limit 5\"\"\").show(5)\n",
    "\n",
    "    print(\"--------------Profit by Customer-------------------\")\n",
    "    spark.sql(\"\"\"SELECT year, customer_id,customer_name, round(SUM(profit),2) AS total_profit FROM enriched_orders\n",
    "            GROUP BY year, customer_id, customer_name\n",
    "            ORDER BY year\n",
    "                limit 10\"\"\").show(5)\n",
    "    \n",
    "    print(\"--------------Profit by Customer + Year-------------\")\n",
    "    spark.sql(\"\"\"SELECT year, customer_id,customer_name,round(SUM(profit),2) AS total_profit FROM enriched_orders\n",
    "            GROUP BY year, customer_id, customer_name\n",
    "            ORDER BY year\n",
    "            limit 10;\"\"\").show(5)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a52960d-3e94-4141-9bf6-8aef0984550e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dbutils.fs.rm(\"dbfs:/user/hive/warehouse/product\", recurse=True)\n",
    "# dbutils.fs.rm(\"dbfs:/user/hive/warehouse/customer\", recurse=True)\n",
    "# dbutils.fs.rm(\"dbfs:/user/hive/warehouse/order\", recurse=True)\n",
    "#dbutils.fs.rm(\"dbfs:/user/hive/warehouse/enriched_orders\", recurse=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1932795331188184,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
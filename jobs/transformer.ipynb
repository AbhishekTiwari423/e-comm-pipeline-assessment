{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e1ef387-2341-410b-975c-67571c6522f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# jobs/transformer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, round as spark_round,sum, min,max,countDistinct,when,avg\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        self.spark = spark\n",
    "\n",
    "    def transform_orders(self, orders_df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transforms the orders DataFrame by:\n",
    "        - Converting 'Order_Date' and 'Ship_Date' to date type\n",
    "        - Casting 'Profit' to double type\n",
    "\n",
    "        Parameters:\n",
    "            orders_df (DataFrame): Input DataFrame with raw order data\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: Transformed DataFrame with updated schema\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If DataFrame is invalid or required columns are missing\n",
    "            Exception: For unexpected transformation errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate input DataFrame\n",
    "            if orders_df is None or not isinstance(orders_df, DataFrame):\n",
    "                raise ValueError(\"Invalid DataFrame provided.\")\n",
    "\n",
    "            required_cols = [\"order_date\", \"ship_date\", \"profit\"]\n",
    "            missing_cols = [c for c in required_cols if c not in orders_df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing required columns: {', '.join(missing_cols)}\")\n",
    "\n",
    "            # Apply transformations\n",
    "            transformed_df = (\n",
    "                orders_df\n",
    "                .withColumn(\"order_date\", to_date(col(\"order_date\"), \"d/M/yyyy\"))\n",
    "                .withColumn(\"ship_date\", to_date(col(\"ship_date\"), \"d/M/yyyy\"))\n",
    "                .withColumn(\"profit\", col(\"profit\").cast(DoubleType()))\n",
    "            )\n",
    "\n",
    "            return transformed_df\n",
    "\n",
    "        except ValueError as ve:\n",
    "            raise Exception(f\"ValueError error during order enrichment: {ve}\") from ve\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Unexpected error during order transformation: {e}\") from e\n",
    "\n",
    "    def enrich_orders(self, orders: DataFrame, customers: DataFrame, products: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Enriches orders by joining with customers and products datasets.\n",
    "\n",
    "        Parameters:\n",
    "            orders (DataFrame): Orders DataFrame (must contain order_id, order_date, ship_date, customer_id, product_id, profit)\n",
    "            customers (DataFrame): Customers DataFrame (must contain customer_id, customer_name, country)\n",
    "            products (DataFrame): Products DataFrame (must contain product_id, category, sub_category)\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: Enriched DataFrame with customer and product information.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If any input DataFrame is invalid or required columns are missing.\n",
    "            Exception: For unexpected join or transformation errors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate input DataFrames\n",
    "            for df, name in [(orders, \"orders\"), (customers, \"customers\"), (products, \"products\")]:\n",
    "                if df is None or not isinstance(df, DataFrame):\n",
    "                    raise ValueError(f\"Invalid DataFrame provided for {name}.\")\n",
    "\n",
    "            # Required columns for each DataFrame\n",
    "            required_cols = {\n",
    "                \"orders\": [\"order_id\", \"order_date\", \"ship_date\", \"customer_id\", \"product_id\", \"profit\"],\n",
    "                \"customers\": [\"customer_id\", \"customer_name\", \"country\"],\n",
    "                \"products\": [\"product_id\", \"category\", \"sub_category\"],\n",
    "            }\n",
    "\n",
    "            # Check missing columns\n",
    "            for df, name in [(orders, \"orders\"), (customers, \"customers\"), (products, \"products\")]:\n",
    "                missing_cols = [c for c in required_cols[name] if c not in df.columns]\n",
    "                if missing_cols:\n",
    "                    raise ValueError(f\"Missing required columns in {name} DataFrame: {', '.join(missing_cols)}\")\n",
    "\n",
    "            # Perform joins and select required fields\n",
    "            enriched_df = (\n",
    "                orders.alias(\"o\")\n",
    "                .join(customers.alias(\"c\"), col(\"o.customer_id\") == col(\"c.customer_id\"), \"inner\")\n",
    "                .join(products.alias(\"p\"), col(\"o.product_id\") == col(\"p.product_id\"), \"inner\")\n",
    "                .select(\n",
    "                    col(\"o.order_id\"),\n",
    "                    col(\"order_date\"),\n",
    "                    col(\"ship_date\"),\n",
    "                    col(\"c.customer_id\"),\n",
    "                    col(\"c.customer_name\"),\n",
    "                    col(\"c.country\"),\n",
    "                    col(\"o.product_id\"),\n",
    "                    col(\"p.category\"),\n",
    "                    col(\"p.sub_category\"),\n",
    "                    spark_round(col(\"profit\"), 2).alias(\"profit\"),\n",
    "                    year(col(\"order_date\")).alias(\"year\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "            return enriched_df\n",
    "\n",
    "        except ValueError as ve:\n",
    "            # Validation errors (bad schema, missing columns, invalid DataFrame)  \n",
    "            # should be raised as-is so caller can handle them explicitly.\n",
    "            raise Exception(f\"ValueError error during order enrichment: {ve}\") from ve\n",
    "        except Exception as e:\n",
    "            # Any unexpected Spark or join errors are wrapped with context for debugging.\n",
    "            raise Exception(f\"Unexpected error during order enrichment: {e}\") from e\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def validate_dataframe(self,df: DataFrame, expected_cols: list, df_name: str):\n",
    "        \"\"\"Ensure DataFrame has required columns.\"\"\"\n",
    "        if not isinstance(df, DataFrame):\n",
    "            raise TypeError(f\"{df_name} must be a Spark DataFrame, got {type(df)}\")\n",
    "        \n",
    "        missing = [col for col in expected_cols if col not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"{df_name} is missing required columns: {missing}\")\n",
    "\n",
    "\n",
    "    def create_enriched_customer_table(self,customers_df: DataFrame, \n",
    "                                    orders_df: DataFrame, \n",
    "                                    products_df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Create an enriched customer table with aggregated metrics.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate schemas\n",
    "            self.validate_dataframe(customers_df, \n",
    "                [\"customer_id\", \"customer_name\", \"email\", \"phone\", \"address\", \n",
    "                \"segment\", \"country\", \"city\", \"state\", \"postal_code\", \"region\"], \n",
    "                \"customers_df\")\n",
    "            \n",
    "            self.validate_dataframe(orders_df, \n",
    "                [\"order_id\", \"order_date\", \"ship_date\", \"ship_mode\", \"customer_id\", \n",
    "                \"product_id\", \"quantity\", \"price\", \"discount\", \"profit\"], \n",
    "                \"orders_df\")\n",
    "            \n",
    "            self.validate_dataframe(products_df, \n",
    "                [\"product_id\", \"category\", \"sub_category\", \"product_name\", \"state\", \"price_per_product\"], \n",
    "                \"products_df\")\n",
    "\n",
    "            # Join orders with products\n",
    "            orders_products = orders_df.join(products_df, \"product_id\", \"left\")\n",
    "\n",
    "            # Aggregate per customer\n",
    "            customer_metrics = (\n",
    "                orders_products.groupBy(\"customer_id\")\n",
    "                .agg(\n",
    "                    countDistinct(\"order_id\").alias(\"total_orders\"),\n",
    "                    sum(\"quantity\").alias(\"total_quantity\"),\n",
    "                    spark_round(sum((F.col(\"price\") * F.col(\"quantity\")) - F.col(\"discount\")),2).alias(\"total_spent\"),\n",
    "                    spark_round(sum(\"discount\"),2).alias(\"total_discount\"),\n",
    "                    spark_round(sum(\"profit\"),2).alias(\"total_profit\"),\n",
    "                    spark_round((sum((F.col(\"price\") * F.col(\"quantity\")) - F.col(\"discount\")) /\n",
    "                    when(F.countDistinct(\"order_id\") != 0, F.countDistinct(\"order_id\"))\n",
    "                    ),2).alias(\"avg_order_value\"),\n",
    "                    min(\"order_date\").alias(\"first_order_date\"),\n",
    "                    max(\"order_date\").alias(\"last_order_date\"),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Add favorite category/sub_category (works on Spark <3.4 with workaround)\n",
    "            fav_cat = (\n",
    "                orders_products.groupBy(\"customer_id\", \"category\")\n",
    "                .count()\n",
    "                .withColumn(\"rank\", F.rank().over(Window.partitionBy(\"customer_id\").orderBy(F.desc(\"count\"))))\n",
    "                .filter(\"rank = 1\")\n",
    "                .drop(\"rank\", \"count\")\n",
    "                .withColumnRenamed(\"category\", \"fav_category\")\n",
    "            )\n",
    "\n",
    "            fav_subcat = (\n",
    "                orders_products.groupBy(\"customer_id\", \"sub_category\")\n",
    "                .count()\n",
    "                .withColumn(\"rank\", F.rank().over(Window.partitionBy(\"customer_id\").orderBy(F.desc(\"count\"))))\n",
    "                .filter(\"rank = 1\")\n",
    "                .drop(\"rank\", \"count\")\n",
    "                .withColumnRenamed(\"sub_category\", \"fav_sub_category\")\n",
    "            )\n",
    "\n",
    "            # Final enriched customers\n",
    "            enriched_customers = (\n",
    "                customers_df.join(customer_metrics, \"customer_id\", \"left\")\n",
    "                .join(fav_cat, \"customer_id\", \"left\")\n",
    "                .join(fav_subcat, \"customer_id\", \"left\")\n",
    "            )\n",
    "\n",
    "            return enriched_customers\n",
    "\n",
    "        except (AnalysisException, ValueError, TypeError) as e:\n",
    "            raise RuntimeError(f\"Error creating enriched customer table: {str(e)}\")\n",
    "\n",
    "    def create_enriched_product_table(self,customers_df: DataFrame, \n",
    "                                    orders_df: DataFrame, \n",
    "                                    products_df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Create an enriched product table with aggregated metrics.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate schemas\n",
    "            self.validate_dataframe(customers_df, [\"customer_id\", \"region\"], \"customers_df\")\n",
    "            self.validate_dataframe(orders_df, \n",
    "                [\"order_id\", \"order_date\", \"customer_id\", \"product_id\", \"quantity\", \"price\", \"discount\", \"profit\"], \n",
    "                \"orders_df\")\n",
    "            self.validate_dataframe(products_df, \n",
    "                [\"product_id\", \"category\", \"sub_category\", \"product_name\", \"state\", \"price_per_product\"], \n",
    "                \"products_df\")\n",
    "\n",
    "            # Aggregate per product\n",
    "            product_metrics = (\n",
    "                orders_df.groupBy(\"product_id\")\n",
    "                .agg(\n",
    "                    countDistinct(\"order_id\").alias(\"total_orders\"),\n",
    "                    sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
    "                    spark_round(sum((F.col(\"price\") * F.col(\"quantity\")) - F.col(\"discount\")),2).alias(\"total_revenue\"),\n",
    "                    spark_round(sum(\"profit\"),2).alias(\"total_profit\"),\n",
    "                    spark_round(avg(\"discount\"),2).alias(\"avg_discount\"),\n",
    "                    countDistinct(\"customer_id\").alias(\"distinct_customers\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Best region per product\n",
    "            product_region = (\n",
    "                orders_df.join(customers_df, \"customer_id\", \"left\")\n",
    "                .groupBy(\"product_id\", \"region\")\n",
    "                .agg(F.sum(\"profit\").alias(\"region_profit\"))\n",
    "            )\n",
    "\n",
    "            best_region = (\n",
    "                product_region\n",
    "                .withColumn(\"rank\", F.rank().over(Window.partitionBy(\"product_id\").orderBy(F.desc(\"region_profit\"))))\n",
    "                .filter(\"rank = 1\")\n",
    "                .drop(\"rank\", \"region_profit\")\n",
    "                .withColumnRenamed(\"region\", \"best_region\")\n",
    "            )\n",
    "\n",
    "            # Final enriched products\n",
    "            enriched_products = (\n",
    "                products_df\n",
    "                .join(product_metrics, \"product_id\", \"left\")\n",
    "                .join(best_region, \"product_id\", \"left\")\n",
    "            )\n",
    "\n",
    "            return enriched_products\n",
    "        except (AnalysisException, ValueError, TypeError) as e:\n",
    "            raise RuntimeError(f\"Error creating enriched product table: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c87c49f5-946e-4b54-8f29-5c7d472a43ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "transformer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
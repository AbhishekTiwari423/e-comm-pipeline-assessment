{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db78d74-ebf2-45b1-afb1-7b340eecd419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../jobs/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35ad551-413f-4f1d-b6f2-f1e4edb913e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 59630), raddr=('127.0.0.1', 41927)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 34148), raddr=('127.0.0.1', 42747)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n../usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 58182), raddr=('127.0.0.1', 40967)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 38462), raddr=('127.0.0.1', 45127)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 43580), raddr=('127.0.0.1', 35699)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 58022), raddr=('127.0.0.1', 33563)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 39598), raddr=('127.0.0.1', 33601)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 54992), raddr=('127.0.0.1', 34047)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 47918), raddr=('127.0.0.1', 45725)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 37484), raddr=('127.0.0.1', 42509)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 50136), raddr=('127.0.0.1', 35947)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n../usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 35042), raddr=('127.0.0.1', 35933)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n.\n----------------------------------------------------------------------\nRan 16 tests in 16.749s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "# tests/test_transformer\n",
    "import unittest\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructField, IntegerType, StringType,StructType\n",
    "import pytest\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "class TestTransformer(unittest.TestCase):\n",
    "    # Schemas for later use\n",
    "    orders_schema = StructType([\n",
    "        StructField(\"order_id\", IntegerType(), True),\n",
    "        StructField(\"order_date\", StringType(), True),\n",
    "        StructField(\"ship_date\", StringType(), True),\n",
    "        StructField(\"customer_id\", IntegerType(), True),\n",
    "        StructField(\"product_id\", IntegerType(), True),\n",
    "        StructField(\"profit\", DoubleType(), True),\n",
    "    ])\n",
    "\n",
    "    customers_schema = StructType([\n",
    "        StructField(\"customer_id\", IntegerType(), True),\n",
    "        StructField(\"customer_name\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "    products_schema = StructType([\n",
    "        StructField(\"product_id\", IntegerType(), True),\n",
    "        StructField(\"category\", StringType(), True),\n",
    "        StructField(\"sub_category\", StringType(), True),\n",
    "])\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.spark = SparkSession.builder.appName(\"TransformerTest\").getOrCreate()\n",
    "        cls.transformer = Transformer(spark)\n",
    "\n",
    "    def test_transform_orders_date_parse(self):\n",
    "        data = [(\"1\", \"1/1/2020\", \"2/1/2020\", 100.5)]\n",
    "        df = self.spark.createDataFrame(data, [\"order_date\", \"ship_date\", \"profit\"])\n",
    "        result = self.transformer.transform_orders(df)\n",
    "        self.assertIn(\"order_date\", result.columns)\n",
    "        self.assertIn(\"ship_date\", result.columns)\n",
    "\n",
    "\n",
    "    def test_valid_transformation(self):\n",
    "        data = [(\"1/2/2023\", \"5/2/2023\", \"100.50\")]\n",
    "        df = self.spark.createDataFrame(data, [\"order_date\", \"ship_date\", \"profit\"])\n",
    "        result = self.transformer.transform_orders(df).collect()[0]\n",
    "\n",
    "        assert str(result[\"order_date\"]) == \"2023-02-01\"\n",
    "        assert str(result[\"ship_date\"]) == \"2023-02-05\"\n",
    "        assert abs(result[\"profit\"] - 100.50) < 0.001\n",
    "\n",
    "    def test_multiple_rows(self):\n",
    "        data = [\n",
    "            (\"1/1/2023\", \"2/1/2023\", \"50\"),\n",
    "            (\"10/12/2022\", \"15/12/2022\", \"75.25\")\n",
    "        ]\n",
    "        df = self.spark.createDataFrame(data, [\"order_date\", \"ship_date\", \"profit\"])\n",
    "        result = self.transformer.transform_orders(df).collect()\n",
    "\n",
    "        assert len(result) == 2\n",
    "        assert str(result[0][\"order_date\"]) == \"2023-01-01\"\n",
    "        assert result[1][\"profit\"] == 75.25\n",
    "\n",
    "\n",
    "    # negative test case\n",
    "    def test_missing_column(self):\n",
    "        data = [(\"1/1/2023\", \"100\")]\n",
    "        df = self.spark.createDataFrame(data, [\"order_date\", \"profit\"])  # Missing Ship_Date\n",
    "\n",
    "        with pytest.raises(Exception):  \n",
    "            self.transformer.transform_orders(df).collect()\n",
    "    \n",
    "    def test_invalid_date_format(self):\n",
    "        data = [(\"2023-01-01\", \"2023-01-02\", \"200\")]\n",
    "        df = self.spark.createDataFrame(data, [\"order_date\", \"ship_date\", \"profit\"])\n",
    "        result = self.transformer.transform_orders(df).collect()[0]\n",
    "\n",
    "        # Invalid format → Spark can't parse → returns None\n",
    "        assert result[\"order_date\"] is None\n",
    "        assert result[\"ship_date\"] is None\n",
    "\n",
    "    def test_non_numeric_profit(self):\n",
    "        data = [(\"1/1/2023\", \"2/1/2023\", \"abc\")]\n",
    "        df = self.spark.createDataFrame(data, [\"order_date\", \"ship_date\", \"profit\"])\n",
    "        result = self.transformer.transform_orders(df).collect()[0]\n",
    "\n",
    "        # Casting fails → becomes null\n",
    "        assert result[\"profit\"] is None\n",
    "\n",
    "    #-------------Edge cases--------------\n",
    "    def test_empty_dataframe(self):\n",
    "        schema = StructType([\n",
    "        StructField(\"order_date\", StringType(), True),\n",
    "        StructField(\"ship_date\", StringType(), True),\n",
    "        StructField(\"profit\", StringType(), True)])\n",
    "        df = self.spark.createDataFrame([], schema=schema)\n",
    "        result = self.transformer.transform_orders(df)\n",
    "        assert result.count() == 0\n",
    "\n",
    "    def test_null_values(self):\n",
    "        schema = StructType([\n",
    "        StructField(\"order_date\", StringType(), True),\n",
    "        StructField(\"ship_date\", StringType(), True),\n",
    "        StructField(\"profit\", StringType(), True)])\n",
    "\n",
    "        data = [(None, \"2/1/2023\", \"100\")]\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "        result = self.transformer.transform_orders(df).collect()[0]\n",
    "\n",
    "        assert result[\"order_date\"] is None\n",
    "        assert str(result[\"ship_date\"]) == \"2023-01-02\"\n",
    "        assert result[\"profit\"] == 100.0\n",
    "\n",
    "    def test_enrich_orders_left_join(self):\n",
    "        orders = self.spark.createDataFrame([(\"O1\", \"C1\", \"P1\", 50.0,\"1/1/2020\",\"1/1/2020\")], [\"order_id\", \"customer_id\", \"product_id\", \"profit\",\"order_date\",\"ship_date\"])\n",
    "        customers = self.spark.createDataFrame([(\"C1\", \"Alice\", \"India\")], [\"customer_id\", \"customer_name\", \"country\"])\n",
    "        products = self.spark.createDataFrame([(\"P1\", \"Furniture\", \"Chair\")], [\"product_id\", \"category\", \"sub_category\"])\n",
    "\n",
    "        result = self.transformer.enrich_orders(orders, customers, products)\n",
    "        self.assertEqual(result.count(), 1)\n",
    "        # result.show(truncate=False)\n",
    "        self.assertEqual(result.first()[\"customer_name\"], \"Alice\")\n",
    "\n",
    "\n",
    "    #--------------normal standard case--------------------\n",
    "    def test_standard_enrichment(self):\n",
    "        orders = spark.createDataFrame(\n",
    "            [(1, \"2023-01-01\", \"2023-01-05\", 100, 200, 123.456)],\n",
    "            schema=self.orders_schema,\n",
    "        )\n",
    "        customers = spark.createDataFrame(\n",
    "            [(100, \"Alice\", \"USA\")],\n",
    "            schema=self.customers_schema,\n",
    "        )\n",
    "        products = spark.createDataFrame(\n",
    "            [(200, \"Electronics\", \"Mobile\")],\n",
    "            schema=self.products_schema,\n",
    "        )\n",
    "\n",
    "        result = self.transformer.enrich_orders(orders, customers, products).collect()[0]\n",
    "\n",
    "        assert result[\"order_id\"] == 1\n",
    "        assert result[\"customer_name\"] == \"Alice\"\n",
    "        assert result[\"country\"] == \"USA\"\n",
    "        assert result[\"category\"] == \"Electronics\"\n",
    "        assert result[\"sub_category\"] == \"Mobile\"\n",
    "        assert result[\"profit\"] == 123.46   # rounded\n",
    "        assert result[\"year\"] == 2023\n",
    "\n",
    "    #----------multiple orders case ------------------- \n",
    "    def test_multiple_orders(self):\n",
    "        orders = self.spark.createDataFrame(\n",
    "            [\n",
    "                (1, \"2023-01-01\", \"2023-01-05\", 100, 200, 10.123),\n",
    "                (2, \"2022-02-01\", \"2022-02-03\", 101, 201, 20.456),\n",
    "            ],\n",
    "            schema=self.orders_schema,\n",
    "        )\n",
    "        customers = self.spark.createDataFrame(\n",
    "            [(100, \"Alice\", \"USA\"), (101, \"Bob\", \"Canada\")],\n",
    "            schema=self.customers_schema,\n",
    "        )\n",
    "        products = self.spark.createDataFrame(\n",
    "            [(200, \"Electronics\", \"Mobile\"), (201, \"Furniture\", \"Chair\")],\n",
    "            schema=self.products_schema,\n",
    "        )\n",
    "\n",
    "        result = self.transformer.enrich_orders( orders, customers, products).collect()\n",
    "\n",
    "        assert len(result) == 2\n",
    "        assert set([row[\"customer_name\"] for row in result]) == {\"Alice\", \"Bob\"}\n",
    "        assert set([row[\"category\"] for row in result]) == {\"Electronics\", \"Furniture\"}\n",
    "\n",
    "    #--------------negative test case-------------------\n",
    "    def test_missing_customer(self):\n",
    "        orders = self.spark.createDataFrame(\n",
    "            [(1, \"2023-01-01\", \"2023-01-05\", 999, 200, 50.0)],\n",
    "            schema=self.orders_schema,\n",
    "        )\n",
    "        customers = self.spark.createDataFrame([], schema=self.customers_schema)  # empty\n",
    "        products = self.spark.createDataFrame([(200, \"Electronics\", \"Laptop\")], schema=self.products_schema)\n",
    "\n",
    "        result = self.transformer.enrich_orders( orders, customers, products).collect()[0]\n",
    "\n",
    "        assert result[\"customer_name\"] is None\n",
    "        assert result[\"country\"] is None\n",
    "        assert result[\"category\"] == \"Electronics\"\n",
    "\n",
    "    def test_missing_product(self):\n",
    "        orders = self.spark.createDataFrame(\n",
    "            [(1, \"2023-01-01\", \"2023-01-05\", 100, 999, 75.0)],\n",
    "            schema=self.orders_schema,\n",
    "        )\n",
    "        customers = spark.createDataFrame([(100, \"Charlie\", \"UK\")], schema=self.customers_schema)\n",
    "        products = spark.createDataFrame([], schema=self.products_schema)  # empty\n",
    "\n",
    "        result = self.transformer.enrich_orders(orders, customers, products).collect()[0]\n",
    "\n",
    "        assert result[\"customer_name\"] == \"Charlie\"\n",
    "        assert result[\"category\"] is None\n",
    "        assert result[\"sub_category\"] is None\n",
    "\n",
    "    #----------------empty order test--------------------\n",
    "    def test_empty_orders(self):\n",
    "        orders = self.spark.createDataFrame([], schema=self.orders_schema)\n",
    "        customers = self.spark.createDataFrame([(100, \"Alice\", \"USA\")], schema=self.customers_schema)\n",
    "        products = self.spark.createDataFrame([(200, \"Electronics\", \"TV\")], schema=self.products_schema)\n",
    "\n",
    "        result = self.transformer.enrich_orders(orders, customers, products)\n",
    "\n",
    "        assert result.count() == 0\n",
    "\n",
    "    #-------------without profit data--------------------\n",
    "    def test_null_profit(self):\n",
    "        orders = self.spark.createDataFrame(\n",
    "            [(1, \"2023-01-01\", \"2023-01-05\", 100, 200, None)],\n",
    "            schema=self.orders_schema,\n",
    "        )\n",
    "        customers = self.spark.createDataFrame([(100, \"Alice\", \"USA\")], schema=self.customers_schema)\n",
    "        products = self.spark.createDataFrame([(200, \"Electronics\", \"Mobile\")], schema=self.products_schema)\n",
    "\n",
    "        result = self.transformer.enrich_orders( orders, customers, products).collect()[0]\n",
    "\n",
    "        assert result[\"profit\"] is None\n",
    "\n",
    "    #------------with null date----------------------\n",
    "    def test_null_dates(self):\n",
    "        orders = self.spark.createDataFrame(\n",
    "            [(1, None, None, 100, 200, 50.0)],\n",
    "            schema=self.orders_schema,\n",
    "        )\n",
    "        customers = self.spark.createDataFrame([(100, \"Alice\", \"USA\")], schema=self.customers_schema)\n",
    "        products = self.spark.createDataFrame([(200, \"Electronics\", \"Mobile\")], schema=self.products_schema)\n",
    "\n",
    "        result = self.transformer.enrich_orders(orders, customers, products).collect()[0]\n",
    "\n",
    "        assert result[\"year\"] is None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05bc2b4b-3dd2-47ba-bcb9-b7d98185f0fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "test_transformer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
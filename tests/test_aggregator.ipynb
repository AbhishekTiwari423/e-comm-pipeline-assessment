{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a8837c1-d0ea-4f09-9e18-f79573122d21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../jobs/aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1539335-7151-4cc8-befe-e166fe8dd066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 60608), raddr=('127.0.0.1', 39005)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 59546), raddr=('127.0.0.1', 45007)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 42342), raddr=('127.0.0.1', 44251)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 32952), raddr=('127.0.0.1', 38761)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n../usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 43726), raddr=('127.0.0.1', 40187)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n.../usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 55730), raddr=('127.0.0.1', 46871)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 34826), raddr=('127.0.0.1', 43939)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 37804), raddr=('127.0.0.1', 40153)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 52146), raddr=('127.0.0.1', 44383)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 54040), raddr=('127.0.0.1', 38909)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 55264), raddr=('127.0.0.1', 44761)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 42192), raddr=('127.0.0.1', 46217)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 55684), raddr=('127.0.0.1', 46193)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 54064), raddr=('127.0.0.1', 42757)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 45138), raddr=('127.0.0.1', 46647)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n.\n----------------------------------------------------------------------\nRan 18 tests in 9.330s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from pyspark.sql import SparkSession\n",
    "import pytest\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType,StringType\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "class TestAggregator(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.spark = SparkSession.builder.master(\"local[1]\").appName(\"AggregatorTest\").getOrCreate()\n",
    "        cls.aggregator = Aggregator(spark)\n",
    "        data = [(\"2020\", \"Furniture\", \"Chair\", \"C1\", \"Alice\", 100.0),\n",
    "                (\"2020\", \"Furniture\", \"Chair\", \"C1\", \"Alice\", 200.0)]\n",
    "        cls.enriched_orders = cls.spark.createDataFrame(\n",
    "            data, [\"year\", \"category\", \"sub_category\", \"customer_id\", \"customer_name\", \"profit\"]\n",
    "        )\n",
    "    \n",
    "    def test_aggregate_profit(self):\n",
    "        result = self.aggregator.aggregate_profit(self.enriched_orders)\n",
    "        self.assertEqual(result.collect()[0][\"profit_sum\"], 300.0)\n",
    "    \n",
    "    def test_single_year_aggregation(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        data = [(2023, 100.55), (2023, 200.45)]\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "        result = self.aggregator.aggregate_profit_by_year(df).collect()\n",
    "\n",
    "        assert len(result) == 1\n",
    "        assert result[0][\"year\"] == 2023\n",
    "        assert result[0][\"profit_sum\"] == 301.0  # rounded to 2 decimals\n",
    "    \n",
    "    def test_multiple_years_aggregation(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        data = [(2022, 100.123), (2022, 200.456), (2023, 50.789)]\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "        result = {row[\"year\"]: row[\"profit_sum\"] for row in self.aggregator.aggregate_profit_by_year(df).collect()}\n",
    "\n",
    "        assert result[2022] == 300.58  # (100.123 + 200.456) rounded\n",
    "        assert result[2023] == 50.79\n",
    "\n",
    "\n",
    "    def test_negative_and_zero_profits(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        data = [(2023, -50.25), (2023, 0.0), (2023, 100.25)]\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "        result = self.aggregator.aggregate_profit_by_year(df).collect()[0]\n",
    "\n",
    "        assert result[\"profit_sum\"] == 50.0  # (-50.25 + 0 + 100.25)\n",
    "    \n",
    "    # -------------------\n",
    "    # Edge Cases\n",
    "    # -------------------\n",
    "\n",
    "    def test_empty_dataframe(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        df = self.spark.createDataFrame([], schema=schema)\n",
    "\n",
    "        result = self.aggregator.aggregate_profit_by_year(df).collect()\n",
    "\n",
    "        assert result == []\n",
    "\n",
    "\n",
    "    def test_all_null_profits(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        data = [(2023, None), (2023, None)]\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "        result = self.aggregator.aggregate_profit_by_year(df).collect()\n",
    "\n",
    "        # sum of nulls should be null\n",
    "        assert result[0][\"profit_sum\"] is None\n",
    "\n",
    "\n",
    "    def test_all_null_years(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        data = [(None, 100.0), (None, 200.0)]\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "        result = self.aggregator.aggregate_profit_by_year(df).collect()\n",
    "\n",
    "        # GroupBy(None) still creates a single group with None as key\n",
    "        assert result[0][\"year\"] is None\n",
    "        assert result[0][\"profit_sum\"] == 300.0\n",
    "\n",
    "    #--------negative test cases\n",
    "    def test_missing_year_column(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        df = self.spark.createDataFrame([(100.0,), (200.0,)], schema=schema)\n",
    "\n",
    "        with pytest.raises(Exception):\n",
    "            self.aggregator.aggregate_profit_by_year(None, df).collect()\n",
    "\n",
    "\n",
    "    def test_invalid_profit_type(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "            StructField(\"profit\", IntegerType(), True),  # not DoubleType\n",
    "        ])\n",
    "        df = self.spark.createDataFrame([(2023, 100), (2023, 200)], schema=schema)\n",
    "\n",
    "        # should still work, Spark can sum integers\n",
    "        result = self.aggregator.aggregate_profit_by_year( df).collect()[0]\n",
    "\n",
    "        assert result[\"profit_sum\"] == 300.0\n",
    "\n",
    "\n",
    "    def test_single_customer(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"customer_id\", StringType(), True),\n",
    "            StructField(\"customer_name\", StringType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        data = [(\"C1\", \"Alice\", 100.55), (\"C1\", \"Alice\", 200.45)]\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "        result = self.aggregator.aggregate_profit_by_customer(df).collect()\n",
    "\n",
    "        assert len(result) == 1\n",
    "        assert result[0][\"customer_id\"] == \"C1\"\n",
    "        assert result[0][\"profit_sum\"] == 301.0\n",
    "\n",
    "\n",
    "    def test_multiple_customers(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"customer_id\", StringType(), True),\n",
    "            StructField(\"customer_name\", StringType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        data = [(\"C1\", \"Alice\", 100.0), (\"C2\", \"Bob\", 200.0)]\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "        result = {row[\"customer_id\"]: row[\"profit_sum\"] for row in self.aggregator.aggregate_profit_by_customer(df).collect()}\n",
    "\n",
    "        assert result[\"C1\"] == 100.0\n",
    "        assert result[\"C2\"] == 200.0\n",
    "\n",
    "\n",
    "    def test_rounding(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"customer_id\", StringType(), True),\n",
    "            StructField(\"customer_name\", StringType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        data = [(\"C1\", \"Alice\", 10.126), (\"C1\", \"Alice\", 20.125)]\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "        result = self.aggregator.aggregate_profit_by_customer(df).collect()\n",
    "        assert result[0][\"profit_sum\"] == 30.25   # rounded to 2 decimals\n",
    "\n",
    "\n",
    "    def test_empty_dataframe(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"customer_id\", StringType(), True),\n",
    "            StructField(\"customer_name\", StringType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        df = self.spark.createDataFrame([], schema=schema)\n",
    "\n",
    "        result = self.aggregator.aggregate_profit_by_customer(df).collect()\n",
    "        assert result == []\n",
    "\n",
    "\n",
    "    def test_zero_profits(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"customer_id\", StringType(), True),\n",
    "            StructField(\"customer_name\", StringType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        data = [(\"C1\", \"Alice\", 0.0), (\"C1\", \"Alice\", 0.0)]\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "        result = self.aggregator.aggregate_profit_by_customer(df).collect()\n",
    "        assert result[0][\"profit_sum\"] == 0.0\n",
    "\n",
    "\n",
    "    def test_missing_columns(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"id\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"value\", DoubleType(), True),\n",
    "        ])\n",
    "        df = self.spark.createDataFrame([(\"1\", \"X\", 10.0)], schema=schema)\n",
    "\n",
    "        with pytest.raises(Exception):  # should fail due to missing customer_id\n",
    "            self.aggregator.aggregate_profit_by_customer(df).collect()\n",
    "\n",
    "\n",
    "    def test_large_profits(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"customer_id\", StringType(), True),\n",
    "            StructField(\"customer_name\", StringType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        data = [(\"C1\", \"Alice\", 1e12), (\"C1\", \"Alice\", 1e12)]\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "        result = self.aggregator.aggregate_profit_by_customer(df).collect()\n",
    "        assert result[0][\"profit_sum\"] == 2e12\n",
    "\n",
    "    # --- Positive Cases ---\n",
    "\n",
    "    def test_valid_aggregation(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "            StructField(\"product_category\", StringType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        data = [\n",
    "            (2023, \"Furniture\", 100.55),\n",
    "            (2023, \"Furniture\", 200.45),\n",
    "            (2023, \"Technology\", 50.0),\n",
    "            (2024, \"Furniture\", 300.0),\n",
    "        ]\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "        result = self.aggregator.aggregate_profit_by_year_and_category(df).collect()\n",
    "\n",
    "        expected = {\n",
    "            (2023, \"Furniture\"): 301.0,   # rounded 100.55 + 200.45\n",
    "            (2023, \"Technology\"): 50.0,\n",
    "            (2024, \"Furniture\"): 300.0,\n",
    "        }\n",
    "\n",
    "        for row in result:\n",
    "            assert expected[(row[\"year\"], row[\"product_category\"])] == row[\"profit_sum\"]\n",
    "\n",
    "\n",
    "    def test_single_group(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "            StructField(\"product_category\", StringType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        df = self.spark.createDataFrame([(2025, \"Office Supplies\", 999.99)], schema=schema)\n",
    "\n",
    "        result = self.aggregator.aggregate_profit_by_year_and_category(df).collect()\n",
    "\n",
    "        assert result[0][\"year\"] == 2025\n",
    "        assert result[0][\"product_category\"] == \"Office Supplies\"\n",
    "        assert result[0][\"profit_sum\"] == 999.99\n",
    "\n",
    "\n",
    "    # --- Edge Cases ---\n",
    "\n",
    "    def test_empty_dataframe(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "            StructField(\"product_category\", StringType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        df = self.spark.createDataFrame([], schema=schema)\n",
    "        result = self.aggregator.aggregate_profit_by_year_and_category(df).collect()\n",
    "\n",
    "        assert result == []\n",
    "\n",
    "\n",
    "    def test_null_profit_values(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "            StructField(\"product_category\", StringType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        data = [(2023, \"Furniture\", None), (2023, \"Furniture\", 100.0)]\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "        result = self.aggregator.aggregate_profit_by_year_and_category(df).collect()\n",
    "\n",
    "        # Should ignore null and just sum valid values\n",
    "        assert result[0][\"profit_sum\"] == 100.0\n",
    "\n",
    "\n",
    "    # --- Negative Cases ---\n",
    "\n",
    "    def test_missing_columns(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "            StructField(\"profit\", DoubleType(), True),\n",
    "        ])\n",
    "        df = self.spark.createDataFrame([(2023, 100.0)], schema=schema)\n",
    "\n",
    "        with pytest.raises(Exception):\n",
    "            result = self.aggregator.aggregate_profit_by_year_and_category(df)\n",
    "            result.collect()  # force evaluation\n",
    "\n",
    "\n",
    "    def test_invalid_profit_type(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"year\", IntegerType(), True),\n",
    "            StructField(\"product_category\", StringType(), True),\n",
    "            StructField(\"profit\", StringType(), True),  # invalid type\n",
    "        ])\n",
    "        data = [(2023, \"Furniture\", \"abc\"), (2023, \"Furniture\", \"xyz\")]\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "        with pytest.raises(Exception):\n",
    "            result = self.aggregator.aggregate_profit_by_year_and_category(df)\n",
    "            result.collect()  # trigger Spark evaluation\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10da0ebc-363c-4645-9fde-b5088636a828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1932795331188208,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "test_aggregator",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
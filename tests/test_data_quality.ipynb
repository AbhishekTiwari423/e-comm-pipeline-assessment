{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7506cd5-f1a4-4796-8162-d3617ab050a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../jobs/data_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4324577c-ed28-4178-836f-e4a82f435694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 38408), raddr=('127.0.0.1', 37673)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 36610), raddr=('127.0.0.1', 34499)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n......../usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 50392), raddr=('127.0.0.1', 40337)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n../usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 58096), raddr=('127.0.0.1', 36903)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n../usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 39796), raddr=('127.0.0.1', 37477)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 35212), raddr=('127.0.0.1', 40649)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 36084), raddr=('127.0.0.1', 44287)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n........."
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while filtering positive values for columns ['value']: Invalid DataFrame provided.\nError while filtering positive values for columns value: Parameter 'col_names' must be a non-empty list.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while filtering records: Missing columns in DataFrame: ['order_date']\nError while filtering records: Missing columns in DataFrame: ['ship_date']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/utils.py:51: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n/databricks/spark/python/pyspark/sql/pandas/utils.py:85: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(pyarrow.__version__) < LooseVersion(minimum_pyarrow_version):\n/usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 42210), raddr=('127.0.0.1', 33845)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n/databricks/spark/python/pyspark/sql/pandas/conversion.py:161: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(pa.__version__) >= LooseVersion(\"13.0.0\"):\n......."
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while filtering positive values for columns ['value']: Invalid DataFrame provided.\nError while filtering positive values for columns ['name']: Column 'name' must be numeric, found type 'string'.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 46156), raddr=('127.0.0.1', 38813)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 60356), raddr=('127.0.0.1', 43419)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 34050), raddr=('127.0.0.1', 33901)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n..../usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 35794), raddr=('127.0.0.1', 35429)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n./usr/lib/python3.11/socket.py:789: ResourceWarning: unclosed <socket.socket fd=80, family=2, type=1, proto=6, laddr=('127.0.0.1', 44338), raddr=('127.0.0.1', 46793)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n.\n----------------------------------------------------------------------\nRan 48 tests in 15.108s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, StructField\n",
    "\n",
    "class TestDataQuality(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.spark = SparkSession.builder.appName(\"test-reader\").getOrCreate()\n",
    "        cls.dq = DataQuality(cls.spark)\n",
    "    \n",
    "    def test_standardize_column_names(self):\n",
    "        input_df = self.spark.createDataFrame([(\"O1\", 100.0)], [\"Order Id\", \"profit-margin\"])\n",
    "        result_df = self.dq.standardize_column_names(input_df)\n",
    "        result_df_cols = result_df.columns\n",
    "        expected_columns = [\"order_id\", \"profit_margin\"]\n",
    "\n",
    "        self.assertEqual(result_df_cols, expected_columns)\n",
    "    \n",
    "    # ---------- POSITIVE CASES ----------\n",
    "    def test_columns_with_spaces(self):\n",
    "        df = self.spark.createDataFrame([(1, \"Alice\")], [\"Order ID\", \"Customer Name\"])\n",
    "        result = self.dq.standardize_column_names(df)\n",
    "        assert result.columns == [\"order_id\", \"customer_name\"]\n",
    "    \n",
    "    def test_columns_with_hyphens(self):\n",
    "        df = self.spark.createDataFrame([(1, \"Alice\")], [\"order-id\", \"ship-date\"])\n",
    "        result = self.dq.standardize_column_names(df)\n",
    "        assert result.columns == [\"order_id\", \"ship_date\"]\n",
    "\n",
    "    def test_columns_with_spaces_and_hyphens(self):\n",
    "        df = self.spark.createDataFrame([(1, \"Alice\",\"24/08/2025\")], [\"Order-ID\", \"Customer Name\", \"Ship-Date\"])\n",
    "        result = self.dq.standardize_column_names(df)\n",
    "        assert result.columns == [\"order_id\", \"customer_name\", \"ship_date\"]\n",
    "\n",
    "    def test_columns_already_clean(self):\n",
    "        df = self.spark.createDataFrame([(1, \"X1-212\")], [\"order_id\", \"customer_id\"])\n",
    "        result = self.dq.standardize_column_names(df)\n",
    "        assert result.columns == [\"order_id\", \"customer_id\"]\n",
    "\n",
    "    # ---------- EDGE CASES ----------\n",
    "    def test_empty_dataframe(self):\n",
    "        schema = StructType([])\n",
    "        df = self.spark.createDataFrame([], schema=schema)\n",
    "        result = self.dq.standardize_column_names(df)\n",
    "        assert result.columns == []\n",
    "\n",
    "    def test_multiple_consecutive_spaces_and_hyphens(self):\n",
    "        df = self.spark.createDataFrame([(1, \"Alice\")], [\"Order  ID\", \"Ship--Date\"])\n",
    "        result = self.dq.standardize_column_names(df)\n",
    "        assert result.columns == [\"order__id\", \"ship__date\"]\n",
    "\n",
    "    def test_leading_trailing_spaces_and_hyphens(self):\n",
    "        df = self.spark.createDataFrame([(1, \"Alice\")], [\" Order \", \"-Customer-\"])\n",
    "        result = self.dq.standardize_column_names(df)\n",
    "        assert result.columns == [\"_order_\", \"_customer_\"]\n",
    "\n",
    "    def test_duplicate_columns_after_transform(self):\n",
    "        df = self.spark.createDataFrame([(1, \"Alice\")], [\"Order-ID\", \"Order ID\"])\n",
    "        with pytest.raises(Exception):  # duplicate after transform (\"order_id\")\n",
    "            sel.dq.standardize_column_names(df)\n",
    "\n",
    "    #==========================\n",
    "\n",
    "    def test_filter_not_null(self):\n",
    "        data = [\n",
    "            (1, \"A\", 100),\n",
    "            (2, None, 200),\n",
    "            (3, \"B\", None),\n",
    "            (4, \"C\", 300)\n",
    "        ]\n",
    "        df = spark.createDataFrame(data, [\"id\", \"name\", \"salary\"])\n",
    "\n",
    "        # Filter out rows where name or salary is null\n",
    "        df_filtered = self.dq.filter_not_null(df, [\"name\", \"salary\"])\n",
    "        self.assertEqual(df_filtered.count(), 2)\n",
    "\n",
    "    def test_filter_single_column_not_null(self):\n",
    "        data = [(1, \"Alice\"), (2, None), (3, \"Charlie\")]\n",
    "        df = self.spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "        result = self.dq.filter_not_null( df, [\"name\"])\n",
    "        assert result.count() == 2\n",
    "        assert [r[\"id\"] for r in result.collect()] == [1, 3]\n",
    "\n",
    "    def test_filter_multiple_columns_not_null(self):\n",
    "        data = [(1, \"Alice\", 100), (2, None, 200), (3, \"Bob\", None)]\n",
    "        df = self.spark.createDataFrame(data, [\"id\", \"name\", \"salary\"])\n",
    "        result = self.dq.filter_not_null(df, [\"name\", \"salary\"])\n",
    "        assert result.count() == 1\n",
    "        assert result.collect()[0][\"id\"] == 1\n",
    "\n",
    "    def test_no_nulls_present(self):\n",
    "        data = [(1, \"Alice\", 100), (2, \"Bob\", 200)]\n",
    "        df = self.spark.createDataFrame(data, [\"id\", \"name\", \"salary\"])\n",
    "        result = self.dq.filter_not_null(df, [\"name\", \"salary\"])\n",
    "        assert result.count() == 2\n",
    "\n",
    "    def test_empty_dataframe(self):\n",
    "        schema = \"id INT, name STRING\"\n",
    "        df = self.spark.createDataFrame([], schema=schema)\n",
    "        result = self.dq.filter_not_null(df, [\"name\"])\n",
    "        assert result.count() == 0\n",
    "    \n",
    "    def test_all_rows_null_in_filtered_column(self):\n",
    "        data = [(1, None), (2, None)]\n",
    "        schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "        ])\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "        result = self.dq.filter_not_null( df, [\"name\"])\n",
    "        assert result.count() == 0\n",
    "\n",
    "    def test_filter_with_no_columns_passed(self):\n",
    "        data = [(1, \"Alice\"), (2, None)]\n",
    "        df = self.spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "        result = self.dq.filter_not_null(df, [])\n",
    "        # No filter applied → return same count\n",
    "        assert result.count() == 2\n",
    "\n",
    "    def test_some_columns_not_in_dataframe(self):\n",
    "        data = [(1, \"Alice\", 100)]\n",
    "        df = self.spark.createDataFrame(data, [\"id\", \"name\", \"salary\"])\n",
    "        with pytest.raises(Exception):  # should fail because col doesn't exist\n",
    "            self.dq.filter_not_null(df, [\"missing_col\"])\n",
    "\n",
    "\n",
    "    # ---------- NEGATIVE CASES ----------\n",
    "    def test_none_as_dataframe(self):\n",
    "        with pytest.raises(AttributeError):\n",
    "            self.dq.filter_not_null( None, [\"col1\"])\n",
    "\n",
    "    def test_invalid_column_list_type(self):\n",
    "        data = [(1, \"Alice\")]\n",
    "        df = self.spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "        with pytest.raises(TypeError):\n",
    "            self.dq.filter_not_null(df, \"name\")  # string instead of list\n",
    "\n",
    "    def test_invalid_column_name_type(self):\n",
    "        data = [(1, \"Alice\")]\n",
    "        df = self.spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "        with pytest.raises(Exception):  # int is not a valid col name\n",
    "            self.dq.filter_not_null( df, [123])\n",
    "\n",
    "\n",
    "    def test_filter_invalid_contact(self):\n",
    "        data = [\n",
    "        (1, \"123-456-7890\"),(2, \"(123) 456-7890\"),(3, \"+1 123 456 7890\"),\n",
    "        (4, \"1234567890\"),(5, \"123-45-6789\"), (6, \"12-3456-7890\"),(7, \"phone123456\")]\n",
    "        \n",
    "        df = self.spark.createDataFrame(data, [\"id\", \"phone\"])\n",
    "\n",
    "        valid_contact_df = self.dq.filter_valid_contact(df, \"phone\")\n",
    "        self.assertEqual(valid_contact_df.count(), 4)\n",
    "\n",
    "    # --- positive test cases\n",
    "    def test_valid_us_numbers(self):\n",
    "        data = [(\"123-456-7890\",), (\"(123) 456-7890\",), (\"+1 123 456 7890\",)]\n",
    "        df = self.spark.createDataFrame(data, [\"phone\"])\n",
    "        result = self.dq.filter_valid_contact(df, \"phone\")\n",
    "        # all are valid → should return all 3\n",
    "        assert result.count() == 3\n",
    "        assert set(r[\"phone\"] for r in result.collect()) == {\"123-456-7890\", \"(123) 456-7890\", \"+1 123 456 7890\"}\n",
    "\n",
    "    \n",
    "    def test_valid_numbers_excluded(self):\n",
    "        data = [(\"12345\",), (\"abc-def-ghij\",), (\"999999\",)]\n",
    "        df = self.spark.createDataFrame(data, [\"phone\"])\n",
    "        result = self.dq.filter_valid_contact(df, \"phone\")\n",
    "        # none are valid → expect 0 rows\n",
    "        assert result.count() == 0\n",
    "\n",
    "\n",
    "    def test_mixed_valid_and_invalid(self):\n",
    "        data = [(\"123-456-7890\",), (\"999999\",), (\"(123) 456-7890\",), (\"abc\",)]\n",
    "        df = self.spark.createDataFrame(data, [\"phone\"])\n",
    "        result = self.dq.filter_valid_contact(df, \"phone\")\n",
    "        assert set(result.toPandas()[\"phone\"]) == {\"123-456-7890\", \"(123) 456-7890\"}\n",
    "\n",
    "    #-----Edge cases------------------------\n",
    "    def test_empty_dataframe_with_schema(self):\n",
    "        schema = StructType([StructField(\"phone\", StringType(), True)])\n",
    "        df = self.spark.createDataFrame([], schema)\n",
    "        result = self.dq.filter_valid_contact(df, \"phone\")\n",
    "        assert result.count() == 0\n",
    "\n",
    "\n",
    "    def test_all_null_values(self):\n",
    "        data = [(None,), (None,)]\n",
    "        schema = StructType([StructField(\"phone\", StringType(), True)])\n",
    "        df = self.spark.createDataFrame(data, schema=schema)\n",
    "        result = self.dq.filter_valid_contact(df, \"phone\")\n",
    "        # nulls are not valid → expect 0\n",
    "        assert result.count() == 0\n",
    "\n",
    "\n",
    "    def test_all_valid_numbers(self):\n",
    "        data = [(\"(555) 555-5555\",), (\"555.555.5555\",)]\n",
    "        df = self.spark.createDataFrame(data, [\"phone\"])\n",
    "        result = self.dq.filter_valid_contact(df, \"phone\")\n",
    "        assert result.count() == 2\n",
    "\n",
    "    #-------------negative test cases\n",
    "    def test_non_existent_column(self):\n",
    "        data = [(\"123-456-7890\",)]\n",
    "        df = self.spark.createDataFrame(data, [\"phone\"])\n",
    "        with pytest.raises(Exception):  # Spark AnalysisException\n",
    "            self.dq.filter_valid_contact(df, \"wrong_col\")\n",
    "\n",
    "\n",
    "    def test_invalid_column_type_integer(self):\n",
    "        data = [(1234567890,), (9876543210,)]\n",
    "        df = self.spark.createDataFrame(data, [\"phone\"])\n",
    "        with pytest.raises(Exception):  # rlike requires string type\n",
    "            self.dq.filter_valid_contact(df, \"phone\")\n",
    "\n",
    "\n",
    "    def test_invalid_col_argument_type(self):\n",
    "        data = [(\"123-456-7890\",)]\n",
    "        df = spark.createDataFrame(data, [\"phone\"])\n",
    "        with pytest.raises(TypeError):\n",
    "            self.dq.filter_valid_contact(df, 123)  # col must be str\n",
    "\n",
    "    ### Positive cases ###\n",
    "    def test_clean_valid_names(self):\n",
    "        data = [(\"John123\",), (\"A@nna!!\",), (\"R0bert$\",)]\n",
    "        df = self.spark.createDataFrame(data, [\"name\"])\n",
    "        result_df = self.dq.clean_name_column(df, \"name\")\n",
    "        result = [row[\"name\"] for row in result_df.collect()]\n",
    "        assert result == [\"John\", \"Anna\", \"Rbert\"]\n",
    "\n",
    "    def test_clean_with_spaces(self):\n",
    "        data = [(\" Jo hn \",), (\" Ma ry \",)]\n",
    "        df = self.spark.createDataFrame(data, [\"name\"])\n",
    "        result_df = self.dq.clean_name_column(df, \"name\")\n",
    "        result = [row[\"name\"] for row in result_df.collect()]\n",
    "        assert result == [\"Jo hn\", \"Ma ry\"]\n",
    "\n",
    "\n",
    "    ### Edge cases ###\n",
    "    def test_empty_string(self):\n",
    "        data = [(\"\",)]\n",
    "        df = self.spark.createDataFrame(data, [\"name\"])\n",
    "        result_df = self.dq.clean_name_column(df, \"name\")\n",
    "        result = [row[\"name\"] for row in result_df.collect()]\n",
    "        assert result == [\"\"]\n",
    "\n",
    "    def test_only_special_chars(self):\n",
    "        data = [(\"!@#$%\",)]\n",
    "        df = self.spark.createDataFrame(data, [\"name\"])\n",
    "        result_df = self.dq.clean_name_column(df, \"name\")\n",
    "        result = [row[\"name\"] for row in result_df.collect()]\n",
    "        assert result == [\"\"]\n",
    "\n",
    "    def test_only_numbers(self):\n",
    "        data = [(\"12345\",)]\n",
    "        df = self.spark.createDataFrame(data, [\"name\"])\n",
    "        result_df = self.dq.clean_name_column(df, \"name\")\n",
    "        result = [row[\"name\"] for row in result_df.collect()]\n",
    "        assert result == [\"\"]\n",
    "\n",
    "    def test_null_values(self):\n",
    "        data = [(None,), (\"Valid123\",)]\n",
    "        df = self.spark.createDataFrame(data, [\"name\"])\n",
    "        result_df = self.dq.clean_name_column(df, \"name\")\n",
    "        result = [row[\"name\"] for row in result_df.collect()]\n",
    "        assert result == [None, \"Valid\"]\n",
    "\n",
    "\n",
    "    ### Negative cases ###\n",
    "    def test_invalid_dataframe(self):\n",
    "        with pytest.raises(ValueError):\n",
    "            self.dq.clean_name_column(None, \"name\")\n",
    "\n",
    "    def test_missing_column(self):\n",
    "        data = [(\"John\",)]\n",
    "        df = self.spark.createDataFrame(data, [\"first_name\"])\n",
    "        with pytest.raises(ValueError):\n",
    "            self.dq.clean_name_column(df, \"name\")\n",
    "\n",
    "    def test_non_dataframe_input(self):\n",
    "        with pytest.raises(ValueError):\n",
    "            self.dq.clean_name_column(\"Not a DF\", \"name\")\n",
    "\n",
    "    #------------------Test cases for valid orders---------------\n",
    "    ### Positive cases ###\n",
    "    def test_valid_orders_filtered(self):\n",
    "        data = [\n",
    "            (1, date(2023, 1, 1), date(2023, 1, 5)),  # valid\n",
    "            (2, date(2023, 2, 10), date(2023, 2, 5)), # invalid\n",
    "            (3, date(2023, 3, 1), date(2023, 3, 1)),  # equal\n",
    "            (4, date(2023, 4, 1), date(2023, 4, 3))   # valid\n",
    "        ]\n",
    "        df = self.spark.createDataFrame(data, [\"order_id\", \"order_date\", \"ship_date\"])\n",
    "        result_df = self.dq.filter_valid_orders(df, \"order_date\", \"ship_date\")\n",
    "        result = [row[\"order_id\"] for row in result_df.collect()]\n",
    "        assert set(result) == {1, 4}\n",
    "\n",
    "\n",
    "    def test_all_invalid(self):\n",
    "        data = [\n",
    "            (1, date(2023, 1, 10), date(2023, 1, 5)),\n",
    "            (2, date(2023, 2, 5), date(2023, 2, 5))\n",
    "        ]\n",
    "        df = self.spark.createDataFrame(data, [\"order_id\", \"order_date\", \"ship_date\"])\n",
    "        result_df = self.dq.filter_valid_orders(df, \"order_date\", \"ship_date\")\n",
    "        assert result_df.count() == 0\n",
    "\n",
    "\n",
    "    def test_all_valid(self):\n",
    "        data = [\n",
    "            (1, date(2023, 1, 1), date(2023, 1, 2)),\n",
    "            (2, date(2023, 2, 1), date(2023, 2, 3))\n",
    "        ]\n",
    "        df = self.spark.createDataFrame(data, [\"order_id\", \"order_date\", \"ship_date\"])\n",
    "        result_df = self.dq.filter_valid_orders(df, \"order_date\", \"ship_date\")\n",
    "        assert result_df.count() == 2\n",
    "\n",
    "\n",
    "    ### Edge cases ###\n",
    "    def test_empty_dataframe(self):\n",
    "        df = self.spark.createDataFrame([], \"order_id INT, order_date DATE, ship_date DATE\")\n",
    "        result_df = self.dq.filter_valid_orders(df, \"order_date\", \"ship_date\")\n",
    "        assert result_df.count() == 0\n",
    "\n",
    "\n",
    "    def test_null_values(self):\n",
    "        data = [\n",
    "            (1, None, date(2023, 1, 5)),  # order_date null\n",
    "            (2, date(2023, 2, 1), None),  # ship_date null\n",
    "            (3, None, None)               # both null\n",
    "        ]\n",
    "        df = self.spark.createDataFrame(data, [\"order_id\", \"order_date\", \"ship_date\"])\n",
    "        result_df = self.dq.filter_valid_orders(df, \"order_date\", \"ship_date\")\n",
    "        assert result_df.count() == 0   # null comparisons always False\n",
    "\n",
    "\n",
    "    ### Negative cases ###\n",
    "    def test_invalid_dataframe(self):\n",
    "        with pytest.raises(ValueError):\n",
    "            self.dq.filter_valid_orders(None, \"order_date\", \"ship_date\")\n",
    "\n",
    "\n",
    "    def test_missing_order_column(self):\n",
    "        data = [(1, date(2023, 1, 1))]\n",
    "        df = spark.createDataFrame(data, [\"ship_date\"])\n",
    "        with pytest.raises(ValueError):\n",
    "            self.dq.filter_valid_orders(df, \"order_date\", \"ship_date\")\n",
    "\n",
    "\n",
    "    def test_missing_ship_column(self):\n",
    "        data = [(1, date(2023, 1, 1))]\n",
    "        df = self.spark.createDataFrame(data, [\"order_date\"])\n",
    "        with pytest.raises(ValueError):\n",
    "            self.dq.filter_valid_orders(df, \"order_date\", \"ship_date\")\n",
    "\n",
    "\n",
    "    def test_non_dataframe_input(self):\n",
    "        with pytest.raises(ValueError):\n",
    "            self.dq.filter_valid_orders(\"not_a_dataframe\", \"order_date\", \"ship_date\")\n",
    "\n",
    "    # ---------------Test cases for filtering columns with positive values\n",
    "    ### Positive cases ###\n",
    "    def test_filter_positive_numeric(self):\n",
    "        data = [(1, 100.0), (2, -50.0), (3, 0.0), (4, 25.5)]\n",
    "        df = self.spark.createDataFrame(data, [\"id\", \"price\"])\n",
    "        result_df = self.dq.filter_positive_values(df, [\"price\"])\n",
    "        result = [row[\"id\"] for row in result_df.collect()]\n",
    "        assert set(result) == {1, 4}\n",
    "\n",
    "    def test_filter_positive_integer(self):\n",
    "        data = [(1, 5), (2, -3), (3, 0), (4, 10)]\n",
    "        df = self.spark.createDataFrame(data, [\"id\", \"quantity\"])\n",
    "        result_df = self.dq.filter_positive_values(df, [\"quantity\"])\n",
    "        result = [row[\"id\"] for row in result_df.collect()]\n",
    "        assert set(result) == {1, 4}\n",
    "\n",
    "    def test_all_positive(self):\n",
    "        data = [(1, 10), (2, 20)]\n",
    "        df = self.spark.createDataFrame(data, [\"id\", \"score\"])\n",
    "        result_df = self.dq.filter_positive_values(df, [\"score\"])\n",
    "        assert result_df.count() == 2\n",
    "\n",
    "    def test_all_non_positive(self):\n",
    "        data = [(1, -10), (2, 0)]\n",
    "        df = self.spark.createDataFrame(data, [\"id\", \"score\"])\n",
    "        result_df = self.dq.filter_positive_values(df, [\"score\"])\n",
    "        assert result_df.count() == 0\n",
    "\n",
    "\n",
    "    ### Edge cases ###\n",
    "    def test_empty_dataframe(self):\n",
    "        df = self.spark.createDataFrame([], \"id INT, value INT\")\n",
    "        result_df = self.dq.filter_positive_values(df, [\"value\"])\n",
    "        assert result_df.count() == 0\n",
    "\n",
    "    def test_null_values(self):\n",
    "        data = [(1, None), (2, -5), (3, 10)]\n",
    "        df = self.spark.createDataFrame(data, [\"id\", \"value\"])\n",
    "        result_df = self.dq.filter_positive_values(df, [\"value\"])\n",
    "        result = [row[\"id\"] for row in result_df.collect()]\n",
    "        assert result == [3]\n",
    "\n",
    "\n",
    "    ### Negative cases ###\n",
    "    def test_invalid_dataframe(self):\n",
    "        with pytest.raises(ValueError):\n",
    "            self.dq.filter_positive_values(None, [\"value\"])\n",
    "\n",
    "    def test_missing_column(self):\n",
    "        data = [(1, 10)]\n",
    "        df = self.spark.createDataFrame(data, [\"id\"])\n",
    "        with pytest.raises(ValueError):\n",
    "            self.dq.filter_positive_values(df, \"value\")\n",
    "\n",
    "    def test_non_numeric_column(self):\n",
    "        data = [(1, \"abc\"), (2, \"xyz\")]\n",
    "        df = self.spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "        with pytest.raises(ValueError):\n",
    "            self.dq.filter_positive_values(df, [\"name\"])\n",
    "\n",
    "    def test_non_dataframe_input(self):\n",
    "        with pytest.raises(ValueError):\n",
    "            self.dq.filter_positive_values(\"not_a_dataframe\", [\"value\"])\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "test_data_quality",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}